{
  
    
        "post0": {
            "title": "The history of COVID-19 in San Diego County",
            "content": "Table of Contents - click on a link to go there . Software: (don&#39;t care how it&#39;s made? skip ahead!) . Code to grab data from San Diego County website | Code to fit exponential models to the data | . Analyses: . The history of COVID-19 in San Diego | Fraction of cases which result in hospitalization or worse | How many new COVID cases there are today | Doubling times &amp; flattening the curve | Social distancing is what flattened the curve | Introduction . A small (yet worryingly large) group of people are currently protesting the things that are keeping us safe: stay at home orders, and closure orders for non-essential business and recreation. Some of these people are informed and honest, they say &quot;more people will die, but freedom or the economy is more important.&quot; . Some of these people don&#39;t understand fully what&#39;s going on. With this post I&#39;m trying to reach those people. So if you say things like &quot;COVID-19 is not so bad, look how few people have died, we need to end these restrictions&quot; or &quot;We already have herd immunity because it was here for months and months, so we don&#39;t need these restrictions&quot;, then please read and feel free to ask questions. . I&#39;m trying to show that social distancing, the stay at home orders, and school/park/beach closures have saved many lives. Without it San Diego hospitals would be in crisis with thousands more sick and hundreds more dead than we have today. . So if you only read one thing here, please read this analysis showing that social distancing in San Diego has probably saved hundreds of lives. The rest of this notebook may also be useful for you in understanding the severity of the outbreak in San Diego, and how we got there. . Code to scrape the county website . If you&#39;d like to see how this is done, just click on the &quot;Show Code&quot; buttons below. . #collapse import pandas as pd from dateparser.search import search_dates import seaborn as sns import glob from tabula import read_pdf import numpy as np sns.set_style(&#39;darkgrid&#39;) colors = sns.color_palette() import matplotlib.pyplot as plt dataf = &#39;Data/SDcountySituation-{}.csv&#39; locf = &#39;Data/SDcountyLocations-{}.csv&#39; zipf = &#39;Data/SDcountyZipcodes-{}.csv&#39; tab_hist = {} . . #collapse # Data from March 19 to today is now in the repo # Use this cell to load it up if you&#39;re starting a new analysis from scratch files = glob.glob(&#39;Data/SDcountySituation*.csv&#39;) # gets all .csv filenames in directory for afile in files: dt = afile.split(&#39;/&#39;)[1][18:28] tab_hist[dt] = pd.read_csv(afile,index_col=0,header=0) . . #collapse # Run me every day! Website updates once a day at around 4pm tab = pd.read_html(&#39;https://www.sandiegocounty.gov/content/sdc/hhsa/programs/phs/community_epidemiology/dc/2019-nCoV/status.html&#39;) #comes down as a list of each column seperately, with the header the same in each column-table tabs = [ x.drop(0) for x in tab] table = pd.concat(tabs) # setup header and index cs = table.iloc[0,:].to_list() cs[0] = &#39;index&#39; table.columns = cs table = table.drop(1).set_index(&#39;index&#39;) # get todays date from that header we threw away on each column # great, they&#39;ve updated the format again and now it says #&quot;updated Date X with data through Date X-1&quot; # it&#39;s like they&#39;re trying to make it harder for me by changing something every day! # need to go back to Date X format for consistency with previous data # OLD: dt = search_dates(tab[0][0][0])[-1][1] dt = search_dates(tab[0][0][0])[-2][1] today = dt.strftime(&#39;%Y-%m-%d&#39;) . . #collapse # Starting 26 March the Total and non-county residents columns disappeared # and everythign is under San Diego County Residents, even though I believe that is actually the Total column # if i&#39;m wrong this generates a discontinuity of 20 non-resident cases, making the growth from 25 - 26 March look # smaller than it is, it will lead to an underestimate of the exponential coefficient # Update: I was wrong. There is an underestimate of the early doubling time built into the data. Now after a few days of seeing county briefs I understand that thye changed how they count... people&#39;s registered address with DMV is used to count the numbers... not where they actually lived or where they visited the hospital. # and each jurisdiction is now only counting its residents (in this sense of the word), not the people in its hospitals table.columns=[&#39;Total&#39;] # record todays data for posterity table.to_csv(dataf.format(today)) tab_hist[today] = table print(today) table . . 2020-04-20 . Total . index . Total Positives | 2325 | . Age Groups | NaN | . 0-9 years | 17 | . 10-19 years | 43 | . 20-29 years | 354 | . 30-39 years | 431 | . 40-49 years | 398 | . 50-59 years | 438 | . 60-69 years | 318 | . 70-79 years | 178 | . 80+ years | 144 | . Age Unknown | 4 | . Gender | NaN | . Female | 1149 | . Male | 1172 | . Unknown | 4 | . Hospitalizations | 562 | . Intensive Care | 189 | . Deaths | 72 | . #collapse cases = pd.Series({key:int(value.loc[&#39;Total Positives&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() hospitalizations = pd.Series({key:int(value.loc[&#39;Hospitalizations&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() icu = pd.Series({key:int(value.loc[&#39;Intensive Care&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() deaths = pd.Series({key:int(value.loc[&#39;Deaths&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() totals = pd.DataFrame( {&#39;Confirmed cases&#39;:cases, &#39;Hospitalizations&#39;:hospitalizations, &#39;ICU patients&#39;:icu, &#39;Deaths&#39;:deaths}) . . Code to fit exponential models to the COVID data . The classic model of disease outbreaks is an exponential function. One person infects two. Two infect four. Four infect eight. And in ten easy steps you get to thousands of people, in twenty doubling steps its millions of people infected. . Below is how the suasage is made in the models, click Show Code button to see it. . #collapse import numpy as np from scipy.optimize import curve_fit from scipy.stats.distributions import t def exfunc(x, a, b): &#39;&#39;&#39;This is the form of an exponential function which we will fit to pandemic data&#39;&#39;&#39; return a * np.exp(b * x) def estimate_doubling_time(data, verbose=False): &#39;&#39;&#39; Given some data, fits parameters for an exponential function defined in exfunc() and returns the resulting doubling time along with other useful info Inputs: data - the data points to be fit, e.g. a 5 day window of COVID hospitalization totals Outputs: doubling time - maximum likelihood estimate of exponential doubling time ci95 - 95% confidence interval on that estimate (only 5% chance that the true value is outside doubling_time +/- ci95) pest - maximum likelihood estimate of the exponential parameters a,b defined in exfunc() pcov - covariance matrix of the exponential parameters a,b defined in exfunc() &#39;&#39;&#39; # use Levenberg-Marquardt algorithm to solve for a minimum in the # nonlinear least squares error function (find the best parameters to fit the data) pest, pcov = curve_fit(exfunc, range(len(data)), data) # the above just found the parameters a,b that best fit a*exp(b*x) to the data # the doubling time of an exponential function like the above is just: log(2)/b doubling_time = np.log(2)/pest[1] # next we are going to put a confidence interval on our doubling time alpha = 0.05 # 95% confidence interval = 100*(1-alpha) n = len(data) # number of data points p = len(pest) # number of parameters dof = max(0, n - p) # number of degrees of freedom # Students-t value for the dof and confidence level tval = t.ppf(1.0-alpha/2., dof) # curve_fit() gave a covariance matrix on the fit, turn that into standard deviation of a,b sigmas = np.power( np.diag(pcov), 0.5) # magnitude of the confidence interval is simply Students-t * std deviation of doubling time # so we have to translate from std deviation of parameter b to std deviation of doubling time ci95 = (np.log(2)/(pest[1]-sigmas[1]) - doubling_time)*tval if verbose: # output results of this fit print(&#39;t:{:4.3f} p0:[{:4.3f}+/-{:4.3f}] p1:[{:4.3f}+/-{:4.3f}] dtime:{:4.3f}+/-{:4.3f}&#39;.format( tval, pest[0], sigmas[0], pest[1], sigmas[1], doubling_time, ci95)) return(doubling_time, ci95, pest, pcov) def days_to_value(pest, y): &#39;&#39;&#39; pest = [a, b] y = a * exp( b * x ) ln(y) - ln(a) = b * x x = ( ln(y) - ln(a) ) / b &#39;&#39;&#39; return (np.log(y) - np.log(pest[0]))/pest[1] . . . The history of COVID-19 infections and hospitalizations in San Diego . #collapse totals.plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;) plt.suptitle(&#39;San Diego County COVID-19 history&#39;) plt.savefig(&#39;Plots/chistory.png&#39;,dpi=300); . . . Fraction of confirmed COVID-19 cases that get bad, really bad, and dead . I&#39;m using these graphs to track a few things about the outbreak. First, its good to know how many people who have the disease are ending up in hospital or worse. Secondly, in a later section of this post, I&#39;m using one of these estimates to help predict how many people would be dead now if we didn&#39;t social distance. . However, please don&#39;t think the graph below tells you &quot;how likely am I to die if I get it?&quot;!! The number of confirmed COVID-19 cases is only a fraction of the true number of people infected with the virus. By some estimates most of the people carrying the virus have no symptoms. And many people who have symptoms are not getting tested. If the fraction below says 3% die, it may be much much lower. But right now we can&#39;t know how much lower! . #collapse frac = totals[[&#39;Hospitalizations&#39;,&#39;ICU patients&#39;,&#39;Deaths&#39;]] frac = frac.apply(lambda x: x/totals[&#39;Confirmed cases&#39;]) frac.plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;,color=colors[1:]); plt.suptitle(&#39;To date, fraction of total confirmed COVID-19 cases resulting in ...&#39;); . . #collapse frac = totals[[&#39;ICU patients&#39;,&#39;Deaths&#39;]] frac = frac.apply(lambda x: x/totals[&#39;Hospitalizations&#39;]) frac.plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;,color=colors[1:]); plt.suptitle(&#39;To date, fraction of COVID-19 hospitalizations resulting in ...&#39;); . . . Daily COVID increases in San Diego county . These numbers are the most raw way you can see the curve is flattening. Under normal steep exponential growth you would see daily numbers that are always getting bigger. These are starying the same right now, and maybe falling! . #collapse # Obviously the first date (20th or 21st depending on metric) is ignorable because it contains all previous days # Also I&#39;m pretty sure the -5 hospitalizations on 23rd indicates that some data reclassification happened that day, not that some people were cured and walked out of hospital. totals.diff().rename(lambda x: &#39;Todays new &#39;+x, axis=&#39;columns&#39;).plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;); plt.suptitle(&#39;San Diego County COVID daily increases: nIncreases would be growing exponentially if the curve was not flattening&#39;) plt.savefig(&#39;Plots/cdeltas.png&#39;,dpi=300); . . . Is the curve flattening? . Doubling time is the numbe of days it takes some measurement to double. This is a common way to see how fast a geometric/exponential growth process is taking off. Unfortunately, the early part of an epidemic is indeed a geometric growth process. For or epidemics, small numbers are bad, they mean a steep curve. Big numbers are good, they mean the exponential is flattening out and people are getting sick at a slower rate. . Note that this kind of metric is only good when we are in the first half of epidemic. Once we reach the peak of infections and start to have LESS people sick, this kind of metric becomes useless and my mathematical method will produce incorrect values. . I&#39;m using a sliding 7 day window to see how the doubling time is changing from day to day. I&#39;m using t-statistic estimated 95% confidence intervals on the exponential fit to estimate a range of plausibitlty in the estimates. When you see that April 10th has no overlap with the value on March 27th, you can be very sure that the difference is real, the curve is flattening, and it isn&#39;t just random noise causing this. . TL;DR To see if the outbreak is slowing, we want the doubling time to get quite a bit bigger. And we are seeing that, bigtime!!! We might even be close to a peak. . BUT, remember that we can always climb back onto an upward trajectory if we stop social distancing! Just because you&#39;re halfway down the mountain in your car, do you stop using your brakes? . #collapse # The 27th is the first date I&#39;d trust since that elimantes the big delta of # the 1st data point on I collected on March 20th. print(&#39;Lets look at the growth rate of confirmed cases (positive COVID-19 tests)...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in cases[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, case_p_est, case_p_cov = estimate_doubling_time(cases[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for confirmed COVID-19 cases to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 25.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/casedoubling.png&#39;,dpi=300); . . Lets look at the growth rate of confirmed cases (positive COVID-19 tests)... . #collapse # hospitalizations has a weird negative change on the 23rd, due (I think) to them removing # non-SD county residents from the data who were in hospital here # this negative prbably results in us underestimating the doubling time on 3/27 print(&#39;Lets look at the growth rate of hospitalizations for COVID-19...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in hospitalizations[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, hosp_p_est, hosp_p_cov = estimate_doubling_time(hospitalizations[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for COVID-19 hospitalizations to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 25.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/hospitaldoubling.png&#39;,dpi=300); . . Lets look at the growth rate of hospitalizations for COVID-19... . #collapse # ICU patients print(&#39;Lets look at the growth rate of COVID-19 patients in intensive care beds...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in hospitalizations[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, icu_p_est, icu_p_cov = estimate_doubling_time(icu[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for COVID-19 ICU patients to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 25.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/icudoubling.png&#39;,dpi=300); . . Lets look at the growth rate of COVID-19 patients in intensive care beds... . #collapse # Deaths print(&#39;Lets look at the growth rate of COVID-19 deaths...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in hospitalizations[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, deaths_est, deaths_p_cov = estimate_doubling_time(deaths[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for COVID-19 deaths to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 25.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/hospitaldoubling.png&#39;,dpi=300); . . Lets look at the growth rate of COVID-19 deaths... . . Is social distancing slowing down the growth in COVID cases and hospitalizations? . Yes, in a big way. . San Diego schools closed on March 13th. Governor Newsom ordered Californians to stay at home on March 19th. We know that it takes about 1 week after infection for someone to feel sick, and about 2 weeks for them to get bad enough to need hospitalization according to this link. That means that new cases being reduced by the stay-at-home order wouldn&#39;t happen until after March 26th, and the first reduction in new hospitalizations about April 2nd. . I generally prefer to look at the hospitalization data because it&#39;s the least-wrong measurement we have. The number of confirmed cases via COVID tests is undercounting how many people have the virus. Probabaly badly undercounting it. On the other hand, most of the people who have a severe case of the disease will end up in hospital. So I&#39;m going to use it as my main way to measure the course of the outbreak. . Take a look at the yellow dots and lines on the graph below. See where the COVID hospitalization curve below starts to bend over to a shallower slope? A bit more than 2 wks after the stay at home order! The case curve doesn&#39;t bend a full week before that, but this is likely becuase we still had a huge backlog of tested people waiting for their results back then. . You can also see this bending of the curve in the doubling time graphs in the last section of this notebook: We see the first increases in hospitalization doubling time are on April 4th or so. In the last weeks of March the estimated doubling time was around 3 to 5 days for all metrics: cases, hospitalizations, ICU patients, and deaths. Today the doubling time for these is around 10 - 20 days. . Lets look back again at the graph below: In March the growth rate of cases, hospitalizations, and deaths is steep and steady. In April things start to bend, and today we have ended up with a steady, shallower rate of growth right now. . This graph shows what I want everyone to know: . If there had been no stay at home order, then that late March doubling time would have continued until today. That would mean San Diego would have many times more infections than it currently does. There would be hundreds (or maybe by now, thousands) of more people dead if we had not all stayed at home!! . Social distancing saved lives! . #collapse # this projection is based on doubling time calculated only over the last week of data daysfwd = 60 lcolors = colors[:2]+ colors[3:] fromdate = (pd.Timestamp(today)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] startproj = (pd.Timestamp(today)+pd.Timedelta(&#39;1 day&#39;)).isoformat().split(&#39;T&#39;)[0] daystoproj = (pd.Timestamp(&#39;2020-03-19&#39;) + pd.Timedelta(&#39;{} days&#39;.format(daysfwd)) - pd.Timestamp(fromdate)).days print(&#39;Calculate exponential parameters on data from {} to {}, then project forward using those parameters for {} days&#39;.format(fromdate,today,daystoproj)) print(&#39;Compare with exponential parameters fit on data from 2020-03-19 to 2020-03-26&#39;) doubling_time, ci95, case_p_est, case_p_cov = estimate_doubling_time(cases[fromdate:]) doubling_time, ci95, orig_case_p_est, orig_case_p_cov = estimate_doubling_time(cases[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time, ci95, hosp_p_est, hosp_p_cov = estimate_doubling_time(hospitalizations[fromdate:]) doubling_time, ci95, orig_hosp_p_est, orig_hosp_p_cov = estimate_doubling_time(hospitalizations[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time, ci95, icu_p_est, icu_p_cov = estimate_doubling_time(icu[fromdate:]) doubling_time, ci95, death_p_est, death_p_cov = estimate_doubling_time(deaths[fromdate:]) doubling_time, ci95, orig_death_p_est, orig_death_p_cov = estimate_doubling_time(deaths[&#39;2020-03-19&#39;:&#39;2020-03-31&#39;]) projcases = pd.DataFrame(exfunc(range(0,daystoproj),case_p_est[0],case_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) projhosp = pd.DataFrame(exfunc(range(0,daystoproj),hosp_p_est[0],hosp_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) projdeath = pd.DataFrame(exfunc(range(0,daystoproj),death_p_est[0],death_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) projcases.columns = [&#39;Cases projected today&#39;] projhosp.columns = [&#39;Hospitalized projected today&#39;] projdeath.columns = [&#39;Deaths projected today&#39;] projected = pd.merge( pd.merge(projcases, projhosp,left_index=True,right_index=True,how=&#39;outer&#39;), projdeath,left_index=True,right_index=True,how=&#39;outer&#39;) projected = projected[startproj:] orig_projcases = pd.DataFrame(exfunc(range(0,daysfwd),orig_case_p_est[0],orig_case_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projhosp = pd.DataFrame(exfunc(range(0,daysfwd),orig_hosp_p_est[0],orig_hosp_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projdeath = pd.DataFrame(exfunc(range(0,daysfwd),orig_death_p_est[0],orig_death_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projcases.columns = [&#39;Cases projected on 2020-03-27&#39;] orig_projhosp.columns = [&#39;Hospitalized projected on 2020-03-27&#39;] orig_projdeath.columns = [&#39;Deaths projected on 2020-03-27&#39;] orig_projected = pd.merge(pd.merge(orig_projcases,orig_projhosp, left_index=True,right_index=True,how=&#39;outer&#39;), orig_projdeath,left_index=True,right_index=True,how=&#39;outer&#39;) orig_projected = orig_projected[&#39;2020-03-27&#39;:] actual = pd.merge( pd.merge(cases.rename(&#39;Confirmed cases history&#39;), hospitalizations.rename(&#39;Hospitalizations history&#39;), left_index=True,right_index=True,how=&#39;outer&#39; ), deaths.rename(&#39;Deaths history&#39;), left_index=True,right_index=True,how=&#39;outer&#39; ) actual.index = pd.DatetimeIndex(actual.index) fig, ax = plt.subplots(figsize=(10, 5)) projected.plot( title=(&#39;Without social distancing San Diego would have a much worse COVID-19 problem today n&#39; + &#39;Projections from March 27 hadn &#39;t yet shown the effects of social distancing and have steeper slopes than today &#39;s projections. n&#39; #&#39;Each projection is based on 1 week of data preceeding the date the projection is made&#39; ).format( actual.loc[today,&#39;Confirmed cases history&#39;], orig_projected.loc[today,&#39;Cases projected on 2020-03-27&#39;]) ,color=lcolors,ax=ax,logy=True,linestyle=&#39;--&#39;); orig_projected.plot(ax=ax,logy=True,color=lcolors,linestyle=&#39;-.&#39;,alpha=0.33) actual.plot(ax=ax,logy=True,color=lcolors,marker=&#39;.&#39;,linestyle=&#39;&#39;); lims = list(plt.ylim()) lims[1] = 2.0*10**5 plt.ylim(lims) print(&quot; n nIf we didn&#39;t social distance: nThe difference between 3/27 case projections and todays cases = {}&quot;.format( orig_projected.loc[today,&#39;Cases projected on 2020-03-27&#39;] - actual.loc[today,&#39;Confirmed cases history&#39;] )) print(&quot;The difference between 3/27 hospitalized projections and todays hospitalizations = {}&quot;.format( orig_projected.loc[today,&#39;Hospitalized projected on 2020-03-27&#39;] - actual.loc[today,&#39;Hospitalizations history&#39;] )) print(&quot;The difference between 3/27 deaths projections and todays deaths = {:1d}&quot;.format( orig_projected.loc[today,&#39;Deaths projected on 2020-03-27&#39;] - actual.loc[today,&#39;Deaths history&#39;] )) vals = orig_projected.loc[today].rename(&#39;Projected on 2020-03-27&#39;).to_frame() vals.index = [&#39;Cases&#39;, &#39;Hospitalized&#39;, &#39;Deaths&#39;] dos = actual.loc[today].rename(&#39;Today &#39;s actual numbers&#39;) dos.index = [&#39;Cases&#39;, &#39;Hospitalized&#39;, &#39;Deaths&#39;] vals = vals.join(dos) xmn = ax.get_xbound()[0] ax.annotate( vals.T.to_string(), xy=(0.4,0.07), xycoords=&#39;axes fraction&#39;, fontsize=10, family=&#39;monospace&#39;) plt.ylabel(&#39;Number of people&#39;); plt.legend(loc=&#39;upper left&#39;, ncol =3, fontsize=9)#, bbox_to_anchor=(0., .5, 1., 0.5)) plt.savefig(&#39;Plots/socialdistancing.png&#39;,dpi=300); . . Calculate exponential parameters on data from 2020-04-14 to 2020-04-20, then project forward using those parameters for 34 days Compare with exponential parameters fit on data from 2020-03-19 to 2020-03-26 If we didn&#39;t social distance: The difference between 3/27 case projections and todays cases = 13566 The difference between 3/27 hospitalized projections and todays hospitalizations = 4345 The difference between 3/27 deaths projections and todays deaths = 745 . Let me help you understand the graph: I used SD County&#39;s own counts of COVID-19 patients. The two prediction lines are made by fitting exponential parameters to the number of people hospitalized with COVID19 from March 19 - 26 and to hospitalizations in the last week. Note that these graphs have a y-axis which is logarithmic: it goes up 10, 100, 1000, 10000, etc... . In the early stages of an epidemic, the numbers grow exponentially: 1 becomes 2 becomes 4, becomes 8, 16, 32, etc. In 10 steps there&#39;s &gt;1,000. In 20 steps its &gt; 1,000,000. Something that has this kind of exponential growth curve on a linear plot becomes a straight line on a plot with a logarithmic y-axis. . And you can see in this plot that there are essentially two &quot;straight lines&quot; in each metric being tracked. Things have a steeper sloped line in March, and a gentler slope for the line in April. That shows you that we were on track to have a much worse outbreak in March, but something changed. . And as I said above, the thing that changed was the social distancing and various shutdowns. And the timing of the change, being almost exactly 2 weeks afterwards, suggests that the stay at home orders and shutdowns are the major reason for the good news. . I will note one caveat about the death projections above. Back in mid March we didn&#39;t have enough death data to get a really good estimate of the growth rate by 3/26. In fact, the death data projection includes data through all of March to get the slope to try to get a better estimate. Because of this trouble, you could also consider an alternate method for projecting deaths: Just take. the current death rate of people in the hospital for COVID-19 (about 12%) and multiply it by the projected hospitalizations from March 26th. This yields a slighly less bad projection on COVID-19 deaths today: . #collapse print(&#39;Alternate prediction of dead COVID-19 patients today without social distancing:&#39;) print(&#39;{:d}&#39;.format(int( orig_projhosp.loc[today] * frac[&#39;Deaths&#39;].iloc[-1]))) . . Alternate prediction of dead COVID-19 patients today without social distancing: 628 . But that number is still bad, and most importantly, as soon as we have more patients in hospital with COVID-19 then we can reasonably treat then the percentage of people who will die from this disease is going to go up quickly. Which is what I&#39;ll address in teh next section... . The number of available hospital beds . &quot;As of 2018, San Diego had 6,180 total hospital beds. Of those, about 68% were occupied at any given time. That leaves 1,950 beds, including 800 intensive care unit beds, available for new patients.&quot; - https://www.kpbs.org/news/2020/mar/20/data-suggests-san-diego-hospitals-will-be-overfill/ . Update: In early April the federal government started the process to add 250 beds to the unused upper floors of Palomar Hosptial. Also in the weeks since the article above, all hospitals have been postponing elective procedures and generally doing what they can to free up beds. You would also expect a lower rate of accidents with the reductions in work and traffic. . I do not have any sources to describe the current number of free beds in San Diego county but I expect it is 100s if not 1000s more than it was when the March 20th article was written. Things that have happened to increase bed capacity: . Hospitals have tried to reduce utilization by cancelling elective procedures | 250 beds are being added to Palomar Hospital on the upper floors in the next weeks | UCSD has designated an unknown number of beds in their dorms as an alternate site to host recovering COVID patients to free up beds for the people just beginning to get sick | . All this will lead me to make two different hospital capacity predictions: . an optimistic one that assumes federal hospitals will help (like Navy), a huge increase in free beds based on going from 50% free beds to 70% free beds, and 500 additional beds from Palomar and UCSD. | a pessimistic one based soley on free SD county beds from March 20th | . #collapse # Pessimistic number of county only (ignores federal) beds/icu beds available on March 20th from kpbs article above availbeds = 1950 availicu = 800 # optimistic bed capacity based on # 1) all beds including federal beds in navy hospitals (https://www.sandiegouniontribune.com/news/health/story/2020-03-16/regions-hospitals-could-face-thousands-of) # 2) reduce hospital utilization from 50% to 30% by cancelling procedures # 3) increase of 250 beds from palomar hospital expansion # 3) add 250 beds from UCSD alternate care site (WAG from me. There is no statement on how many beds this would be) availbedsO = int(8614*.7 + 500) print(&#39;pessimistic estimate of capacity: 1850 noptimistic estimate of capacity: {}&#39;.format(availbedsO)) . . pessimistic estimate of capacity: 1850 optimistic estimate of capacity: 6529 . #collapse # this projection is based on doubling time calculated only over the last week of data daysfwd = 90 fromdate = (pd.Timestamp(today)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] startproj = (pd.Timestamp(today)+pd.Timedelta(&#39;1 day&#39;)).isoformat().split(&#39;T&#39;)[0] daystoproj = (pd.Timestamp(&#39;2020-03-19&#39;) + pd.Timedelta(&#39;{} days&#39;.format(daysfwd)) - pd.Timestamp(fromdate)).days print(&#39;Calculate exponential parameters on data from {} to {}, then project forward using those parameters for {} days&#39;.format(fromdate,today,daystoproj)) doubling_time, ci95, case_p_est, case_p_cov = estimate_doubling_time(cases[fromdate:]) doubling_time, ci95, orig_case_p_est, orig_case_p_cov = estimate_doubling_time(cases[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time, ci95, hosp_p_est, hosp_p_cov = estimate_doubling_time(hospitalizations[fromdate:]) doubling_time, ci95, orig_hosp_p_est, orig_hosp_p_cov = estimate_doubling_time(hospitalizations[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time, ci95, icu_p_est, icu_p_cov = estimate_doubling_time(icu[fromdate:]) hospital_capacity = days_to_value(hosp_p_est,availbeds) hospital_capacityO = days_to_value(hosp_p_est,availbedsO) orig_hospital_capacity = days_to_value(orig_hosp_p_est,availbeds) orig_hospital_capacityO = days_to_value(orig_hosp_p_est,availbedsO) print(&#39;Before social distancing had enough time to work, hospitals were projected to reach capacity on {}&#39;.format(pd.Timestamp(&#39;2020-03-20&#39;) + pd.Timedelta(&#39;1 day&#39;)*orig_hospital_capacity)) print(&#39;Now I think hospitals reach capacity on {}&#39;.format(pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacity)) print(&#39;Or optimistically they reach capacity on {}&#39;.format(pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacityO)) icu_capacity = days_to_value(icu_p_est, availicu) print(&#39;ICUs reach capacity on {}&#39;.format(pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*icu_capacity)) projhosp = pd.DataFrame(exfunc(range(0,daystoproj),hosp_p_est[0],hosp_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) projhosp.columns = [&#39;Hospitalized projected from today&#39;] projected = projhosp[startproj:] orig_projhosp = pd.DataFrame(exfunc(range(0,daysfwd),orig_hosp_p_est[0],orig_hosp_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projhosp.columns = [&#39;Hospitalized projected on 2020-03-27&#39;] orig_projected = orig_projhosp[&#39;2020-03-27&#39;:] actual = hospitalizations.rename(&#39;History of hospitalizations&#39;) actual.index = pd.DatetimeIndex(actual.index) fig, ax = plt.subplots(figsize=(10, 5)) projected.plot( title=(&#39;Social distancing has shifted the date we could run out of hospital beds backwards by at least a month n&#39;+ &#39;Projections from March 27 hadn &#39;t yet shown the effects of social distancing and have steeper slopes than today &#39;s projections. n&#39; + &#39;Each projection is based on 1 week of data preceeding the date the projection is made&#39; ) ,ax=ax,logy=True,linestyle=&#39;--&#39;,color=colors[1]); orig_projected.plot(ax=ax,logy=True,color=colors[4],linestyle=&#39;-.&#39;,alpha=0.4) actual.plot(ax=ax,logy=True,color=colors[1],marker=&#39;.&#39;,linestyle=&#39;&#39;); ts1 = pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacity ts2 = pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacityO plt.ylabel(&#39;Number of people&#39;); shiftvline = (pd.Timestamp(fromdate) - pd.Timestamp(&#39;2020-03-19&#39;)).days xmn = ax.get_xbound()[0] xmx = (hospital_capacity + xmn + shiftvline) ymn = 0 ymx = availbeds ax.plot([xmn, xmx], [ymx,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.annotate(&#39;Pessimistic hospital capacity&#39;, xy=(xmn+1, 2300), xycoords=&#39;data&#39;, fontsize=10); # original capacity date xmx = orig_hospital_capacity + xmn ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.annotate(&#39;Prediction made before nsocial distancing took effect nApril 15 - April 22&#39;, xy=(xmx-4, 10), xycoords=&#39;data&#39;, fontsize=10); xmn = ax.get_xbound()[0] xmx = (hospital_capacityO + xmn + shiftvline) ymn = 0 ymx = availbedsO ax.plot([xmn, xmx], [ymx,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.annotate(&#39;Optimistic hospital capacity&#39;, xy=(xmn+1, 7200), xycoords=&#39;data&#39;, fontsize=10); ax.annotate(&#39;Current prediction n{} - {}&#39;.format(ts1.strftime(&#39;%B %e&#39;), ts2.strftime(&#39;%B %e&#39;)), xy=(xmx-30, 10), xycoords=&#39;data&#39;, fontsize=10); # original capacity date xmx = orig_hospital_capacityO + xmn ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); lims = list(plt.ylim()) lims[1] = 2.0*10**5 plt.ylim(lims) plt.legend(loc=&#39;best&#39;)#, bbox_to_anchor=(0., .5, 1., 0.5)) plt.savefig(&#39;Plots/hospitalcapacity-compare.png&#39;,dpi=300); . . Calculate exponential parameters on data from 2020-04-14 to 2020-04-20, then project forward using those parameters for 64 days Before social distancing had enough time to work, hospitals were projected to reach capacity on 2020-04-15 14:19:38.844414724 Now I think hospitals reach capacity on 2020-05-26 10:45:20.696810614 Or optimistically they reach capacity on 2020-07-01 07:17:15.998446332 ICUs reach capacity on 2020-06-24 16:32:06.430344295 . When there are more patients than beds, people die more often than if there was good care available (see wha&#39;s happened in NYC and northern Italy). On March 20th KPBS said there were about 1850 available hospital beds for COVID19 patients Probably there are several hundred or maybe a couple of thousand more by now. People aren&#39;t doing elective hospital procedures anymore, and there are at least 250 new beds added to the system as I outlined above. . But assume for a minute that 1850 were all the beds there were. And assume social distancing didn&#39;t happen. Then today we be out of beds and San Diego hospitals would be in crisis... take a look at the graph above... that 1850 number is the pessimistic hospital capacity line. You can see that if we followed the same path we were on in early March, today we would have many more people in hospital than there were available beds. The current situation might be like in hard hit parts of NYC: patients lying in the hallways, left largely unattended because there aren&#39;t enough people to care for them. . Now what about the path we are now? The doubling time of hospitlaization is much slower right now, but it&#39;s still not flat. But we have flattened the curve, and bought ourselves enough time to add hospital capacity so we can avoid a NYC situation. When the projections are flat, horizontal lines then we will be able to breathe a sigh of relief. That&#39;s the point at which its safe to relax the stay at home orders and social distancing in small steps. . And yet even then we must be careful. It is clear that the disease could come roaring back into our region again. We must be vigilant, and ready to put into place some control measures once again if necessary. . N.B. My hospital capacity estimates might be optimistic. . I ran my estimates by two people who actually know stuff. . My summary: Hospitals could be stressed and not dealing well with the infected as soon as they have a couple of patients. They will then adjust and just make-do as best they can, shutting down services, moving resources from here to there as much as they can. These decisions will be made differently by each hospital so there&#39;s really no way to predict hospital capacity. . Perhaps you can see my capacity limit estimate as an upper limit... that is the hospital system in San Diego will likely be in crisis well before my estimate of late May (NB: originally this was mid April, but doubling time is slowing dramatically now!) . To read the actual exchange, which took place on March 26th, see below . --Jason . Gerald Pao, an MD/PhD who&#39;s worked in virology... . Although this is a reasonable estimate at face value it does not take into account how a hospital works You cannot have the highly contagious population in the same space as the rest of the patients who are there for other reasons. So you need to ask how many barrier nursing negative pressure beds does the hospital have? The answer is for any normal hospital the number of beds is between 5-10 I have asked my friends in CA from the Bay area, LA and SD and no one has given me a number higher than 10 This means people will have to designate hospital wings to this task and somehow separate things. In places in Spain and Italy they sometimes designated particular hospitals for this task or had the ER designated for it and rearranged it. In other ones they had particular floors closed off etc. so the real capacity will depend on what the hospital administrators decide to do and will be made in uneven and unpredictable ways as there is no single set of rules on how to manage this. Therefore the pressure on hospitals and when you will be out of capacity will be difficult to estimate. We do not have a national healthcare system as you know so it’s gonna be a a free for all. . Gerald Pao . The Salk Institute for Biological Studies . Joel Wertheim, a bioinformatician who specializes in the evolution and epidemiology of HIV... . Hi Jason, . Interesting stuff. . Another point to consider in the general design is the time-lag between all of these states. Most people are not admitted to the hospital for quite some time after infection. If they proceed to the ICU and/or death, that also can take weeks after initial infection/diagnosis. I’m not sure of the proportion of cases in San Diego diagnosed after admittance to the hospital, as opposed to be people who aren&#39;t (yet) sick enough to be hospitalized. . Also, at UC San Diego, negative pressure rooms were originally used for COVID patients (back when we were treating the first patients brought back from China). Now, they are just being used for procedures that would result in high likelihood of spread (i.e., intubation). So the number of beds can change with the severity of the epidemic (as Gerald also mentioned). . Cheers, . Joel . University of California San Diego .",
            "url": "https://jasongfleischer.github.io/argo-navis/covid19/jupyter/epidemiology/2020/04/20/SanDiego-COVID-19.html",
            "relUrl": "/covid19/jupyter/epidemiology/2020/04/20/SanDiego-COVID-19.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Buzz Aldrin's lifetime mean altitude",
            "content": "At some point online I saw someone calculate Neil Armstrong&#39;s mean lifetime altitude. This seems like a complete Randall Monroe of xkcd.com kind of thing. But I cannot for the life of me find the link now to put it here. . Anyway I needed a silly toy problem to test the Jupyter notebook to blog post capability of fastpages now that I&#39;m using it. So here I am... extending the Armstrong problem to the case of a still-alive astronaut who&#39;s lifetime mean altitude is slowly decreasing. . # per wiki he graduated USMA in 1951 and by 1972 at retirement. I made up the graduation date # he already had 3500 hours flying time per https://www.nationalaviation.org/our-enshrinees/aldrin-buzz/ # assume 9km average flight altitude (~30kft) AFstart = pd.Timestamp(&#39;May 1, 1951 12:00:00 UTC&#39;) AFend = pd.Timestamp(&#39;July 1, 1971 12:00:00 UTC&#39;) AFdur = AFend - AFstart AFeffectiveAlt = 9 * pd.Timedelta(&#39;3500 hours&#39;) / AFdur # spread flight hours over the whole AF career evenly # Gemini 12 https://en.wikipedia.org/wiki/Gemini_12 # assume mean orbital distance of apogee and perigee G12start = pd.Timestamp(&#39;November 11, 1966, 20:46:33 UTC&#39;) G12end = pd.Timestamp(&#39;November 15, 1966, 19:21:04 UTC&#39;) G12meanAlt = np.mean([160,270]) # already in km from wiki # Apollo 11 https://airandspace.si.edu/sites/default/files/images/5317h.jpg # https://history.nasa.gov/SP-4029.pdf # Table 7.11 of https://www.hq.nasa.gov/alsj/a11/a11MIssionReport_1971015566.pdf MoonMeanDistKm = 384400 A11liftoff = pd.Timestamp(&#39;July 16, 1969, 13:32:00 UTC&#39;) A11EarthOrbitAlt = 100 * NMi2Km # this is rough A11TLI = A11liftoff + pd.Timedelta(&#39;2 hours 50 minutes&#39;) A11MidcourseOut = A11liftoff + pd.Timedelta(&#39;26 hours 45 minutes&#39;) A11MidcourseAlt = 109475 * NMi2Km A11LunarInsert = A11liftoff + pd.Timedelta(&#39;75 hours 50 minutes&#39;) A11TEI = A11liftoff + pd.Timedelta(&#39;135 hours 24 minutes&#39;) A11splashdown = A11liftoff + pd.Timedelta(&#39;195 hours 18 minutes&#39;) # lets check to see if we need to do any detailed interpolation, or if mean is enough... # print(109475 * NMi2Km / MoonMeanDistKm, (A11MidcourseOut - A11TLI) / (A11LunarInsert - A11MidcourseOut)) # &gt; 0.5274393860561915 0.4872665534804754 # this is close enough to linear interpolation that I&#39;m not going to bother with anything else # so for the duration of the flight from earth to moon, just fill in 1/2 the lunar distance baidx = pd.date_range(start=&#39;January 20, 1930&#39;,end=&#39;January 20, 2020&#39;,freq=&#39;H&#39;) BuzzAlt = pd.Series(0,index=baidx) BuzzAlt[AFstart:AFend] = AFeffectiveAlt BuzzAlt[G12start:G12end] = G12meanAlt BuzzAlt[A11liftoff:A11TLI] = A11EarthOrbitAlt BuzzAlt[A11TLI:A11LunarInsert] = MoonMeanDistKm / 2 BuzzAlt[A11LunarInsert:A11TEI] = MoonMeanDistKm BuzzAlt[A11TEI:A11splashdown] = MoonMeanDistKm / 2 . BuzzAlt.plot(logy=True, ylim=(1,1000000)); plt.ylabel(&#39;kilometers&#39;); plt.xlabel(&#39;on this date&#39;); plt.title(&#39;Aldrin &#39;s altitude history&#39;); plt.annotate(&#39;Gemini 12&#39;, xy=(G12start,2*10**2), xycoords=&#39;data&#39;, xytext=(-55,0), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Apollo 11&#39;, xy=(A11liftoff,4*10**5), xycoords=&#39;data&#39;, xytext=(5,0), textcoords=&#39;offset points&#39;); . BuzzAlt.expanding().mean().plot(logy=True,ylim=(0.0001,1000)); plt.ylabel(&#39;kilometers&#39;); plt.xlabel(&#39;as of this date&#39;); plt.title(&#39;Aldrin &#39;s mean altitude over his lifetime at a given time&#39;); plt.annotate(&#39;Gemini 12&#39;, xy=(G12start,10**-1), xycoords=&#39;data&#39;, xytext=(-55,0), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Apollo 11&#39;, xy=(A11liftoff,10**2), xycoords=&#39;data&#39;, xytext=(-50,0), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Air Force nflight hours&#39;, xy=(AFstart,10**-2), xycoords=&#39;data&#39;, xytext=(-60,-10), textcoords=&#39;offset points&#39;); . # same thing, but linear y-axis instead of log BuzzAlt.expanding().mean().plot(); plt.ylabel(&#39;kilometers&#39;); plt.xlabel(&#39;as of this date&#39;); plt.title(&#39;Aldrin &#39;s mean altitude over his lifetime at a given time&#39;); plt.annotate(&#39;Gemini 12&#39;, xy=(G12start,10**-1), xycoords=&#39;data&#39;, xytext=(-55,10), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Apollo 11&#39;, xy=(A11liftoff,10**2), xycoords=&#39;data&#39;, xytext=(-50,10), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Air Force nflight hours&#39;, xy=(AFstart,10**-2), xycoords=&#39;data&#39;, xytext=(-60,10), textcoords=&#39;offset points&#39;); .",
            "url": "https://jasongfleischer.github.io/argo-navis/funny/jupyter/xkcd/2020/02/29/Buzz-Aldrin's-average-altitude.html",
            "relUrl": "/funny/jupyter/xkcd/2020/02/29/Buzz-Aldrin's-average-altitude.html",
            "date": " • Feb 29, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Publications",
          "content": "L Chow, ENC Manoogian, A Alvear, JG Fleischer, H Thor, K Dietsche, Q Wang, JS Hodges, KS Nair, S Panda, DG Mashek. (In press) Effects of time restricted eating on body composition and metabolic measures in overweight humans: a randomized trial. Obesity. | MJ Wilkinson, ENC Manoogian, A Zadourian, H Loa, S Fakourib, A Shoghib, JG Fleischer, S Navlakha, S Panda, PR Taub. (2019) Ten-hour time-restricted eating reduces weight, blood pressure, and atherogenic lipids in patients with metabolic syndrome. Cell Metabolism,31(1):92-104, https://doi.org/10.1016/j.cmet.2019.11.004 | AT Hutchison, P Regmi, ENC Manoogian, JG Fleischer, GA Wittert, S Panda, LK Heilbronn. (2019) Time-restricted feeding improves glucose tolerance in men at risk of type 2 diabetes: a randomized crossover trial. Obesity https://doi.org/10.1002/oby.22449 | JG Fleischer, R Schulte, HH Tsai, S Tyagi, A Ibarra, MN Shokhirev, L Huang, MW Hetzer, S Navlakha (2018). Predicting age from the transcriptome of human dermal fibroblasts. Genome Biology, 19:221. https://doi.org/10.1186/s13059-018-1599-6 | GP Dunster, L de la Iglesia, M Ben-Hamo, C Nave, JG Fleischer, S Panda, HO de la Iglesia (2018). Sleepmore in Seattle: Later school start times are associated with more sleep and better performance in high school students. Science Advances, 4 (12), eaau6200. https://dx.doi.org/10.1126/sciadv.aau6200 | JL McKinstry, JG Fleischer, Y Chen, WE Gall, GM Edelman (2016). Imagery may arise from associations formed through sensory experience: a network of spiking neurons controlling a robot learns visual sequences in order to perform a mental rotation task. PLOS One, https://dx.doi.org/10.1371/journal.pone.0162155 | JG Fleischer (2014). Persistent activity through multiple mechanisms in a spiking network that solves DMS tasks. Computational and Systems Neuroscience Meeting (COSYNE). | JG Fleischer and JA Gally (2013). A spiking neural network model of working memory can solve delayed match-to-sample tasks and displays a serial position effect. Poster at The Society for Neuroscience Annual Meeting. | JG Fleischer and AE Kozarev (2012). Perceptual grouping and figure-ground segre- gation arising from short-term plasticity in a spiking network. Computational and Systems Neuroscience Meeting (COSYNE) . | JG Fleischer, JL McKinstry, DE Edelman, and GM Edelman (2011). The Case For Using Brain-Based Devices To Study Consciousness. In, JL Krichmar and H Wagatsuma (Eds.), Neuromorphic and Brain-Based Robots: Trends and Perspectives, Cambridge University Press, pp. 303–320. | JG Fleischer and GM Edelman (2009). Brain-based devices: An embodied approach to linking nervous system structure and function to behavior. IEEE Robotics &amp; Automation Magazine, 16(3):33–41. https://dx.doi.org/10.1109/MRA.2009.933621 | JG Fleischer and JL Krichmar (2007). Sensory integration and remapping in a medial temporal lobe model during maze navigation by a brain-based device. Journal of Integrative Neuroscience, 6(3):403–431. http://dx.doi.org/10.1142/S0219635207001568 | JG Fleischer, JA Gally, GM Edelman, and JL Krichmar (2007). Retrospective and prospective responses arising in a modeled hippocampus during maze navigation by a brain-based device. Proceedings of the National Academy of Sciences USA, 104(9):3556–3561. https://doi.org/10.1073/pnas.0611571104 | DA Nitz, WJ Kargo, and JG Fleischer (2007). Dopamine signaling and the distal reward problem. Neuroreport, 18(17):1833–1836. https://doi.org/10.1097/WNR.0b013e3282f16d86 | JG Fleischer (2007). Neural correlates of anticipation in cerebellum, basal ganglia, and hippocampus. In, MV Butz, O Siguard, G Baldassarre, G Pezzulo (Eds.),, Anticipatory Behavior in Adaptive Learning Systems: From Brains to Individual and Social Behavior, Lecture Notes in Artificial Intelligence. vol 4520, pp.19–34. | JG Fleischer, JA Gally, GM Edelman, and JL Krichmar (2007). Different neural pathways lead to journey-dependent and journey-independent place cell activity in an embodied model of hippocampus. Poster at The Society for Neuroscience Annual Meeting. | JG Fleischer, B Szatmary, DB Hutson, DA Moore, JA Snook, GM Edelman, and JL Krichmar (2006). A neurally controlled robot competes and cooperates with humans in Segway Soccer. In Proceedings of the IEEE International Conference on Robotics and Automation, pp.3673–3678 | B Szatmary, JG Fleischer, DB Hutson, DA Moore, JA Snook, GM Edelman, and JL Krichmar (2006). A Segway-based human-robot soccer team. In Proceedings of the IEEE International Conference on Robotics and Automation, pp.4436–4438. | JL Krichmar, AK Seth, DA Nitz, JG Fleischer, and GM Edelman (2005). Spatial navigation and causal analysis in a brain-based device modeling cortical-hippocampal interactions. Neuroinformatics, 3(3):197–222. https://doi.org/10.1385/NI:3:3:197 | JG Fleischer (2004). Imitation is not enough for lexicon learning In Proceedings of the Eighth International Conference on Simulation of Adaptive Behavior, pp.477– 486. | JG Fleischer, SR Marsland, and JL Shapiro (2003). Sensory Anticipation for Autonomous Selection of Robot Landmarks. In, MV Butz, O Siguard, P Gerard (Eds.), Anticipatory Behavior in Adaptive Learning Systems: Foundations, Theories, and Systems, Lecture Notes in Artificial Intelligence. vol 2684, pp.201–221. | JG Fleischer and SR Marsland (2002). Learning to autonomously select landmarks for navigation and communication. In Proceedings of the Seventh International Conference on Simulation of Adaptive Behavior, pp. 151–160. | JG Fleischer and UDF Nehmzow (2001). Towards robots that give each other navigational directions: Learning symbols for perceptual categories. In Proceedings of the 3rd British Conference on Autonomous Mobile Robotics and Autonomous Systems. Dept. of Computer Science, University of Manchester Technical Report UMCS-01-4-1. | JG Fleischer and WO Troxell (1999). Biomimicry as a tool in the design of robotic systems. In Proceedings of the 3rd International Conference on Engineering Design and Automation. Integrated Technology Systems, Prospect, KY. | .",
          "url": "https://jasongfleischer.github.io/argo-navis/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jasongfleischer.github.io/argo-navis/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}