{
  
    
        "post0": {
            "title": "The history of COVID-19 in San Diego County",
            "content": "For at least the next few weeks I will keep this data blog updated with the latest COVID-19 numbers every day. The numbers come from the San Diego County website, and they update in the early evening. This page will update somepoint in the night or early AM the next day. . You can clone the Jupyter notebook powering this blog post from my Github repo. The repo includes this notebook, various plots, and all the data I&#39;ve scraped from the County of San Diego website. I welcome comments in the issues forum or contributions to the software via pull request. . You may also reach me via Twitter @jasongfleischer or email jfleischer@salk.edu . Table of Contents - click on a link to go there . Software: (don&#39;t care how it&#39;s made? skip ahead!) . Code to grab data from San Diego County website | Code to fit exponential models to the data | . Analyses: . The history of COVID-19 in San Diego | Fraction of cases which result in hospitalization or worse | How many new COVID cases there are today | Doubling times &amp; flattening the curve | Social distancing is what flattened the curve | Introduction . A small (yet worryingly large) group of people are currently protesting the things that are keeping us safe: stay at home orders, and closure orders for non-essential business and recreation. Some of these people are informed and honest, they say &quot;more people will die, but freedom or the economy is more important.&quot; . Some of these people don&#39;t understand fully what&#39;s going on. With this post I&#39;m trying to reach those people. So if you say things like &quot;COVID-19 is not so bad, look how few people have died, we need to end these restrictions&quot; or &quot;We already have herd immunity because it was here for months and months, so we don&#39;t need these restrictions&quot;, then please read and feel free to ask questions. . I&#39;m trying to show that social distancing, the stay at home orders, and school/park/beach closures have saved many lives. Without it San Diego hospitals would be in crisis with thousands more sick and hundreds more dead than we have today. . So if you only read one thing here, please read this analysis showing that social distancing in San Diego has probably saved hundreds of lives. The rest of this notebook may also be useful for you in understanding the severity of the outbreak in San Diego, and how we got there. . Code to scrape the county website . If you&#39;d like to see how this is done, just click on the &quot;Show Code&quot; buttons below. . #collapse # If you&#39;re starting a new analysis from scratch, you need to run this cell. # if you&#39;re just pulling new data for today on an already-running notebook, you can skip this step import pandas as pd from dateparser.search import search_dates import seaborn as sns import glob from tabula import read_pdf import numpy as np sns.set_style(&#39;darkgrid&#39;) colors = sns.color_palette() import matplotlib.pyplot as plt dataf = &#39;Data/SDcountySituation-{}.csv&#39; locf = &#39;Data/SDcountyLocations-{}.csv&#39; zipf = &#39;Data/SDcountyZipcodes-{}.csv&#39; tab_hist = {} # This first part was a one time only thing, no longer necessary: # I used the wayback machine to collect the county data for the # dates before I began this project, starting with first day over 100 cases # which was March 19th.. It&#39;s now commented out but left here for posterity # and reference in case you need to scrape. I went by hand through wayback # to select the exact timestamps I used &#39;&#39;&#39;get_historical_data = [ &#39;https://web.archive.org/web/20200320184055/https://www.sandiegocounty.gov/content/sdc/hhsa/programs/phs/community_epidemiology/dc/2019-nCoV/status.html&#39;, &#39;https://web.archive.org/web/20200321141257/https://www.sandiegocounty.gov/content/sdc/hhsa/programs/phs/community_epidemiology/dc/2019-nCoV/status.html&#39;, &#39;https://web.archive.org/web/20200322080211/https://www.sandiegocounty.gov/content/sdc/hhsa/programs/phs/community_epidemiology/dc/2019-nCoV/status.html&#39;, &#39;https://web.archive.org/web/20200323094801/https://www.sandiegocounty.gov/content/sdc/hhsa/programs/phs/community_epidemiology/dc/2019-nCoV/status.html&#39;, &#39;https://web.archive.org/web/20200324003021/https://www.sandiegocounty.gov/content/sdc/hhsa/programs/phs/community_epidemiology/dc/2019-nCoV/status.html&#39; ] for link in get_historical_data: tab = pd.read_html(link) #comes down as a list of each column seperately, with the header the same in each column-table table = tab[2].drop(0) # setup header and index cs = table.iloc[0,:].to_list() cs[0] = &#39;index&#39; table.columns = cs table = table.drop(1).set_index(&#39;index&#39;) # get todays date from that header we threw away on each column dt = search_dates(tab[2].iloc[0,0])[-1][1] today = dt.strftime(&#39;%Y-%m-%d&#39;) # theres some variability in nomenclature to deal with from day to day idx = table.index.to_series() idx[ &#39;Hospitalized&#39; == idx ] = &#39;Hospitalizations&#39; table.index = idx if not &#39;Intensive Care&#39; in idx: table.loc[&#39;Intensive Care&#39;,:] = 0 print(today) # record todays data for posterity table.to_csv(dataf.format(today)) tab_hist[today] = table &#39;&#39;&#39;; # Data from March 19 to today is now in the repo, load it files = glob.glob(&#39;Data/SDcountySituation*.csv&#39;) # gets all .csv filenames in directory for afile in files: dt = afile.split(&#39;/&#39;)[1][18:28] tab_hist[dt] = pd.read_csv(afile,index_col=0,header=0) . . #collapse # Run this cell every day to pull new data from the county! # Website updates once a day at around 4pm tab = pd.read_html(&#39;https://www.sandiegocounty.gov/content/sdc/hhsa/programs/phs/community_epidemiology/dc/2019-nCoV/status.html&#39;) #comes down as a list of each column seperately, with the header the same in each column-table tabs = [ x.drop(0) for x in tab] table = pd.concat(tabs) # setup header and index cs = table.iloc[0,:].to_list() cs[0] = &#39;index&#39; table.columns = cs table = table.drop(1).set_index(&#39;index&#39;) # get todays date from that header we threw away on each column # great, they&#39;ve updated the format again and now it says #&quot;updated Date X with data through Date X-1&quot; # it&#39;s like they&#39;re trying to make it harder for me by changing something every day! # need to go back to Date X format for consistency with previous data # OLD: dt = search_dates(tab[0][0][0])[-1][1] dt = search_dates(tab[0][0][0])[-2][1] today = dt.strftime(&#39;%Y-%m-%d&#39;) # Starting 26 March the Total and non-county residents columns disappeared # and everythign is under San Diego County Residents, even though I believe that is actually the Total column # if i&#39;m wrong this generates a discontinuity of 20 non-resident cases, making the growth from 25 - 26 March look # smaller than it is, it will lead to an underestimate of the exponential coefficient # Update: I was wrong. There is an underestimate of the early doubling time built into the data. Now after a few days of seeing county briefs I understand that thye changed how they count... people&#39;s registered address with DMV is used to count the numbers... not where they actually lived or where they visited the hospital. # and each jurisdiction is now only counting its residents (in this sense of the word), not the people in its hospitals table.columns=[&#39;Total&#39;] # record todays data for posterity table.to_csv(dataf.format(today)) tab_hist[today] = table cases = pd.Series({key:int(value.loc[&#39;Total Positives&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() hospitalizations = pd.Series({key:int(value.loc[&#39;Hospitalizations&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() icu = pd.Series({key:int(value.loc[&#39;Intensive Care&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() deaths = pd.Series({key:int(value.loc[&#39;Deaths&#39;,&#39;Total&#39;]) for key,value in tab_hist.items()}).sort_index() totals = pd.DataFrame( {&#39;Confirmed cases&#39;:cases, &#39;Hospitalizations&#39;:hospitalizations, &#39;ICU patients&#39;:icu, &#39;Deaths&#39;:deaths}) print(&#39;Last update of this blog post was with data for &#39; + today) print(&#39;NOTE: these are cumulative totals... for instance, there are fewer people currently hospitalized than this number, because some of them have recovered or died.&#39;) print(today) table . . Last update of this blog post was with data for 2020-04-26 NOTE: these are cumulative totals... for instance, there are fewer people currently hospitalized than this number, because some of them have recovered or died. 2020-04-26 . Total . index . Total Positives | 3043 | . Age Groups | NaN | . 0-9 years | 23 | . 10-19 years | 57 | . 20-29 years | 449 | . 30-39 years | 566 | . 40-49 years | 528 | . 50-59 years | 598 | . 60-69 years | 393 | . 70-79 years | 231 | . 80+ years | 195 | . Age Unknown | 3 | . Gender | NaN | . Female | 1489 | . Male | 1551 | . Unknown | 3 | . Hospitalizations | 696 | . Intensive Care | 227 | . Deaths | 111 | . Code to fit exponential models to the COVID data . The classic model of disease outbreaks is an exponential function. One person infects two. Two infect four. Four infect eight. And in ten easy steps you get to thousands of people, in twenty doubling steps its millions of people infected. . Below is how the suasage is made in the models, click Show Code button to see it. . #collapse import numpy as np from scipy.optimize import curve_fit from scipy.stats.distributions import t def exfunc(x, a, b): &#39;&#39;&#39;This is the form of an exponential function which we will fit to pandemic data&#39;&#39;&#39; return a * np.exp(b * x) def estimate_doubling_time(data, verbose=False): &#39;&#39;&#39; Given some data, fits parameters for an exponential function defined in exfunc() and returns the resulting doubling time along with other useful info Inputs: data - the data points to be fit, e.g. a 5 day window of COVID hospitalization totals Outputs: doubling time - maximum likelihood estimate of exponential doubling time ci95 - 95% confidence interval on that estimate (only 5% chance that the true value is outside doubling_time +/- ci95) pest - maximum likelihood estimate of the exponential parameters a,b defined in exfunc() pcov - covariance matrix of the exponential parameters a,b defined in exfunc() &#39;&#39;&#39; # use Levenberg-Marquardt algorithm to solve for a minimum in the # nonlinear least squares error function (find the best parameters to fit the data) pest, pcov = curve_fit(exfunc, range(len(data)), data) # the above just found the parameters a,b that best fit a*exp(b*x) to the data # the doubling time of an exponential function like the above is just: log(2)/b doubling_time = np.log(2)/pest[1] # next we are going to put a confidence interval on our doubling time alpha = 0.05 # 95% confidence interval = 100*(1-alpha) n = len(data) # number of data points p = len(pest) # number of parameters dof = max(0, n - p) # number of degrees of freedom # Students-t value for the dof and confidence level tval = t.ppf(1.0-alpha/2., dof) # curve_fit() gave a covariance matrix on the fit, turn that into standard deviation of a,b sigmas = np.power( np.diag(pcov), 0.5) # magnitude of the confidence interval is simply Students-t * std deviation of doubling time # so we have to translate from std deviation of parameter b to std deviation of doubling time ci95 = (np.log(2)/(pest[1]-sigmas[1]) - doubling_time)*tval if verbose: # output results of this fit print(&#39;t:{:4.3f} p0:[{:4.3f}+/-{:4.3f}] p1:[{:4.3f}+/-{:4.3f}] dtime:{:4.3f}+/-{:4.3f}&#39;.format( tval, pest[0], sigmas[0], pest[1], sigmas[1], doubling_time, ci95)) return(doubling_time, ci95, pest, pcov) def days_to_value(pest, y): &#39;&#39;&#39; pest = [a, b] y = a * exp( b * x ) ln(y) - ln(a) = b * x x = ( ln(y) - ln(a) ) / b &#39;&#39;&#39; return (np.log(y) - np.log(pest[0]))/pest[1] . . . The history of COVID-19 infections and hospitalizations in San Diego . #collapse totals.plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;) plt.suptitle(&#39;San Diego County COVID-19 history&#39;) plt.savefig(&#39;Plots/chistory.png&#39;,dpi=300); . . Here&#39;s the same thing but with a logarithmic scale . #collapse totals.plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;,logy=True) plt.suptitle(&#39;San Diego County COVID-19 history&#39;) plt.savefig(&#39;Plots/chistory-log.png&#39;,dpi=300); . . . Fraction of confirmed COVID-19 cases that get bad, really bad, and dead . I&#39;m using these graphs to track a few things about the outbreak. Also, in a later section of this post, I&#39;m using one of these estimates to help predict how many people would be dead now if we didn&#39;t social distance. . Please don&#39;t think the graph below tells you &quot;how likely am I to die if I get iinfected?&quot;!! The number of confirmed COVID-19 cases is only a fraction of the true number of people infected with the virus. By some estimates most of the people carrying the virus have no symptoms. And many people who have symptoms are not getting tested. If the fraction below says 3% die, the true value may be much much lower. But right now we can&#39;t know how much lower! . #collapse frac = totals[[&#39;Hospitalizations&#39;,&#39;ICU patients&#39;,&#39;Deaths&#39;]] frac = frac.apply(lambda x: x/totals[&#39;Confirmed cases&#39;]) frac.plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;,color=colors[1:]); plt.suptitle(&#39;To date, fraction of total confirmed COVID-19 cases resulting in ...&#39;); . . #collapse frac = totals[[&#39;ICU patients&#39;,&#39;Deaths&#39;]] frac = frac.apply(lambda x: x/totals[&#39;Hospitalizations&#39;]) frac.plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;,color=colors[2:]); plt.suptitle(&#39;To date, fraction of COVID-19 hospitalizations resulting in ...&#39;); . . . Daily COVID increases in San Diego county . These numbers are the most raw way you can see the curve is flattening. Under normal steep exponential growth you would see daily numbers that are always getting bigger. These are starying the same right now, and maybe falling! . #collapse # Obviously the first date (20th or 21st depending on metric) is ignorable because it contains all previous days # Also I&#39;m pretty sure the -5 hospitalizations on 23rd indicates that some data reclassification happened that day, not that some people were cured and walked out of hospital. totals.diff().rename(lambda x: &#39;Todays new &#39;+x, axis=&#39;columns&#39;).plot(subplots=True,sharex=True,figsize=(8,10),marker=&#39;.&#39;); plt.suptitle(&#39;San Diego County COVID daily increases: nIncreases would be growing exponentially if the curve was not flattening&#39;) plt.savefig(&#39;Plots/cdeltas.png&#39;,dpi=300); . . . Is the curve flattening? . Doubling time is the number of days it takes some measurement to double. This is a common way to see how fast a geometric/exponential growth process is taking off. Unfortunately, the early part of an epidemic is indeed a geometric growth process. For or epidemics, small numbers are bad, they mean a steep curve, and people are infecting others quickly. Big numbers are good, they mean the exponential is flattening out and people are getting sick at a slower rate. . Note that this kind of metric is only good when we are in the first half of epidemic. Once we reach the peak of infections and start to have LESS people sick, the mathematical method below will produce incorrect values. I will have to manually decide we are past peak, change the exponential to a decreasing exponential, and re-estimate doubling times of decay. Calling the peak itself is a challenge, and I&#39;d be happy to hear peopel&#39;s opinions on principled ways of doing so. . I&#39;m using a sliding 7 day window to see how the doubling time is changing from day to day. I&#39;m using t-statistic estimated 95% confidence intervals on the exponential fit to estimate a range of plausibitlty in the estimates. When you see that April 10th has no overlap with the value on March 27th, you can be very sure that the difference is real, the curve is flattening, and it isn&#39;t just random noise causing this. . TL;DR To see if the outbreak is slowing, we want the doubling time to get quite a bit bigger. And we are seeing that, bigtime!!! . BUT, remember that we can always climb back onto an upward trajectory if we stop social distancing! Just because you&#39;re halfway down the mountain in your car, do you stop using your brakes? . UPDATE: 4/25 Testing capacity is apparently dramatically increasing. The county is saying that the increase in cases is largely due to this, that the proportion of positive results to # of test isn&#39;t changing (~6%) so that the large increases in confirmed cases is just better testing, not an uptick in infection growth rate. In the graphs below you can see however that it is creating (just by definition) a decrease in doubling time. I&#39;m keeping an eye on this... . #collapse # The 27th is the first date I&#39;d trust since that elimantes the big delta of # the 1st data point on I collected on March 20th. print(&#39;Lets look at the growth rate of confirmed cases (positive COVID-19 tests)...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in cases[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, case_p_est, case_p_cov = estimate_doubling_time(cases[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for confirmed COVID-19 cases to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 25.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/casedoubling.png&#39;,dpi=300); . . Lets look at the growth rate of confirmed cases (positive COVID-19 tests)... . #collapse # hospitalizations has a weird negative change on the 23rd, due (I think) to them removing # non-SD county residents from the data who were in hospital here # this negative prbably results in us underestimating the doubling time on 3/27 print(&#39;Lets look at the growth rate of hospitalizations for COVID-19...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in hospitalizations[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, hosp_p_est, hosp_p_cov = estimate_doubling_time(hospitalizations[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for COVID-19 hospitalizations to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 25.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/hospitaldoubling.png&#39;,dpi=300); . . Lets look at the growth rate of hospitalizations for COVID-19... . #collapse # ICU patients print(&#39;Lets look at the growth rate of COVID-19 patients in intensive care beds...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in hospitalizations[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, icu_p_est, icu_p_cov = estimate_doubling_time(icu[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for COVID-19 ICU patients to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 40.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/icudoubling.png&#39;,dpi=300); . . Lets look at the growth rate of COVID-19 patients in intensive care beds... . #collapse # Deaths print(&#39;Lets look at the growth rate of COVID-19 deaths...&#39;) start_calc = &#39;2020-03-27&#39; ex = [] er = [] ix = [] # calc doubling time in a sliding window backwards 1 week for each day for day in hospitalizations[start_calc:].index: todate = day fromdate = (pd.Timestamp(day)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] doubling_time, ci95, deaths_est, deaths_p_cov = estimate_doubling_time(deaths[fromdate:todate]) ex.append( doubling_time ) er.append( ci95 ) ix.append( day[-4:] ) plt.errorbar( ix, ex, yerr=er, fmt=&#39;.&#39;, markersize=15) plt.title(&#39;Time for COVID-19 deaths to double in San Diego County ncalc &#39;d on 1 week sliding window before date: nSharp increase since April 3 shows we are flattening the curve&#39;) plt.ylabel(&#39;Days to double (expected +/- 95%CI)&#39;) plt.xlabel(&#39;Date&#39;) plt.ylim([.5, 25.5]) plt.xticks(rotation=-60) plt.savefig(&#39;Plots/deathdoubling.png&#39;,dpi=300); . . Lets look at the growth rate of COVID-19 deaths... . . Is social distancing slowing down the growth in COVID cases and hospitalizations? . Yes, in a big way. Social distancing has probably saved more than 1,200 lives in San Diego County. . The closures of recreation and businesses have been hard to take, and people are understandably restless. But as we decide what to do, it’s important to make an informed decision. Some have said that COVID-19 is just a flu, that we have overreacted. I want those people to understand why that thinking is wrong. . In the first part of an epidemic, the number of infected people grows exponentially: 2 becomes 4, becomes 8, 16, etc.. In 20 doubling steps more than one million are infected. The speed of doubling (is it 3 or 20 days to double the infected?) comes from how well the infection is spreading among people, which is determined at least partly by our social interactions. . I fit exponential curves to county data on COVID-19. In mid to late March the doubling time was about 4 days. Currently it’s more like 23 days. What changed? . We started social distancing in many different ways, including shutdowns and stay-at-home. The doubling time for COVID-19 hospitalizations started to increase sharply in the days after April 3rd. It takes about two weeks for someone to get sick enough with the virus to end up in the hospital according to this link. Governor Newsom’s stay-at-home order was March 19th, sixteen days before doubling time starts to get much better. This suggests that everyone staying at home dramatically reduced new infections almost immediately. . If we had not used social distancing and stay-at-home, then April would also have had doubling times around 4 days throughout the month. The graph below shows us what that looks like. . Let me explain the plot: Each line on that graph is an exponential doubling time. When an exponential curve is plotted on a logarithmic y-axis (1, 10, 100, etc.) as we have here, it comes out as a straight line. Blue shows confirmed COVID-19 cases via testing, orange shows people hospitalized with the virus, red shows deaths of people who had the virus. Each colored measurement has two lines, one fit on numbers for March 19-26 (faded color dash-dots), and one fit on the numbers from the last week (dashed). The actual numbers recorded every day are shown as circles. . #collapse # this projection is based on doubling time calculated only over the last week of data daysfwd = 60 lcolors = colors[:2]+ colors[3:] fromdate = (pd.Timestamp(today)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] startproj = (pd.Timestamp(today)+pd.Timedelta(&#39;1 day&#39;)).isoformat().split(&#39;T&#39;)[0] daystoproj = (pd.Timestamp(&#39;2020-03-19&#39;) + pd.Timedelta(&#39;{} days&#39;.format(daysfwd)) - pd.Timestamp(fromdate)).days print(&#39;Calculate exponential parameters on data from {} to {}, then project forward using those parameters for {} days&#39;.format(fromdate,today,daystoproj)) print(&#39;Compare with exponential parameters fit on data from 2020-03-19 to 2020-03-26&#39;) doubling_time_case, ci95, case_p_est, case_p_cov = estimate_doubling_time(cases[fromdate:]) doubling_time_case_orig, ci95, orig_case_p_est, orig_case_p_cov = estimate_doubling_time(cases[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time_hosp, ci95, hosp_p_est, hosp_p_cov = estimate_doubling_time(hospitalizations[fromdate:]) doubling_time_hosp_orig, ci95, orig_hosp_p_est, orig_hosp_p_cov = estimate_doubling_time(hospitalizations[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time_icu, ci95, icu_p_est, icu_p_cov = estimate_doubling_time(icu[fromdate:]) doubling_time_death, ci95, death_p_est, death_p_cov = estimate_doubling_time(deaths[fromdate:]) doubling_time_death_orig, ci95, orig_death_p_est, orig_death_p_cov = estimate_doubling_time(deaths[&#39;2020-03-19&#39;:&#39;2020-03-31&#39;]) projcases = pd.DataFrame(exfunc(range(0,daystoproj),case_p_est[0],case_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) projhosp = pd.DataFrame(exfunc(range(0,daystoproj),hosp_p_est[0],hosp_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) projdeath = pd.DataFrame(exfunc(range(0,daystoproj),death_p_est[0],death_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) tday = pd.to_datetime(today).strftime(&quot;%-m/%-d&quot;) projcases.columns = [&#39;Confirmed cases projected on &#39;+tday] projhosp.columns = [&#39;Hospitalized projected on &#39;+tday] projdeath.columns = [&#39;Deaths projected on &#39;+tday] projected = pd.merge( pd.merge(projcases, projhosp,left_index=True,right_index=True,how=&#39;outer&#39;), projdeath,left_index=True,right_index=True,how=&#39;outer&#39;) projected = projected[startproj:] orig_projcases = pd.DataFrame(exfunc(range(0,daysfwd),orig_case_p_est[0],orig_case_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projhosp = pd.DataFrame(exfunc(range(0,daysfwd),orig_hosp_p_est[0],orig_hosp_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projdeath = pd.DataFrame(exfunc(range(0,daysfwd),orig_death_p_est[0],orig_death_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projcases.columns = [&#39;Confirmed cases projected on 3/27&#39;] orig_projhosp.columns = [&#39;Hospitalized projected on 3/27&#39;] orig_projdeath.columns = [&#39;Deaths projected on 3/27&#39;] orig_projected = pd.merge(pd.merge(orig_projcases,orig_projhosp, left_index=True,right_index=True,how=&#39;outer&#39;), orig_projdeath,left_index=True,right_index=True,how=&#39;outer&#39;) orig_projected = orig_projected[&#39;2020-03-27&#39;:] actual = pd.merge( pd.merge(cases.rename(&#39;Confirmed cases history&#39;), hospitalizations.rename(&#39;Hospitalized history&#39;), left_index=True,right_index=True,how=&#39;outer&#39; ), deaths.rename(&#39;Deaths history&#39;), left_index=True,right_index=True,how=&#39;outer&#39; ) actual.index = pd.DatetimeIndex(actual.index) fig, ax = plt.subplots(figsize=(5.94*1.72, 3.76*1.72)) projected.plot(color=lcolors,ax=ax,logy=True,linestyle=&#39;--&#39;); orig_projected.plot(ax=ax,logy=True,color=lcolors,linestyle=&#39;-.&#39;,alpha=0.33) actual.plot(ax=ax,logy=True,color=lcolors,marker=&#39;.&#39;,linestyle=&#39;&#39;); lims = list(plt.ylim()) lims[1] = 2.0*10**5 plt.ylim(lims) lims = list(plt.xlim()) lims[0] = lims[0]-1 plt.xlim(lims) print(&quot; n nIf we didn&#39;t social distance: nThe difference between 3/27 case projections and todays cases = {}&quot;.format( orig_projected.loc[today,&#39;Confirmed cases projected on 3/27&#39;] - actual.loc[today,&#39;Confirmed cases history&#39;] )) print(&quot;The difference between 3/27 hospitalized projections and todays hospitalizations = {}&quot;.format( orig_projected.loc[today,&#39;Hospitalized projected on 3/27&#39;] - actual.loc[today,&#39;Hospitalized history&#39;] )) print(&quot;The difference between 3/27 deaths projections and todays deaths = {:1d}&quot;.format( orig_projected.loc[today,&#39;Deaths projected on 3/27&#39;] - actual.loc[today,&#39;Deaths history&#39;] )) vals = orig_projected.loc[today].rename(tday+&#39; predicted on 3/27&#39;).to_frame() vals.index = [&#39;Cases&#39;, &#39;Hospitalized&#39;, &#39;Deaths&#39;] dos = actual.loc[today].rename(tday+&#39; actual numbers&#39;) dos.index = [&#39;Cases&#39;, &#39;Hospitalized&#39;, &#39;Deaths&#39;] vals = vals.join(dos) xmn = ax.get_xbound()[0] ax.annotate( vals.T.to_string(), xy=(0.21,0.07), xycoords=&#39;axes fraction&#39;, fontsize=14, weight=&#39;heavy&#39;, family=&#39;monospace&#39;) plt.ylabel(&#39;Number of people&#39;, fontsize=14,fontweight=&#39;heavy&#39;); plt.yticks([1,10,100,1000,10000,100000],[1,10,100,&#39;1,000&#39;,&#39;10,000&#39;,&#39;100,000&#39;],fontsize=14,fontweight=&#39;heavy&#39;) for tick in ax.xaxis.get_majorticklabels(): tick.set_size(14) tick.set_weight(&#39;heavy&#39;) for tick in ax.xaxis.get_minorticklabels(): tick.set_size(14) tick.set_weight(&#39;heavy&#39;) # U-T wants no title on plot, jpg format (wut?) plt.legend(loc=&#39;lower left&#39;, bbox_to_anchor = (-0.15,1.), ncol=3, prop={&#39;weight&#39;:&#39;heavy&#39;, &#39;size&#39;:12.25}, labelspacing=0.3, facecolor=&#39;w&#39;, framealpha=0.66)#, bbox_to_anchor=(0., .5, 1., 0.5)) plt.tight_layout() plt.savefig(&#39;Plots/socialdistancing-ut.jpg&#39;,dpi=int(300./1.72)); #plt.title( (&#39;Without social distancing San Diego would have a much worse COVID-19 problem today n&#39; + # &#39;Projections from March 27 hadn &#39;t yet shown the effects of social distancing and have steeper slopes than today &#39;s projections. n&#39; # ).format( # actual.loc[today,&#39;Confirmed cases history&#39;], # orig_projected.loc[today,&#39;Confirmed cases projected on 2020-03-27&#39;]) ) plt.tight_layout() plt.savefig(&#39;Plots/socialdistancing.png&#39;,dpi=300); . . Calculate exponential parameters on data from 2020-04-20 to 2020-04-26, then project forward using those parameters for 28 days Compare with exponential parameters fit on data from 2020-03-19 to 2020-03-26 If we didn&#39;t social distance: The difference between 3/27 case projections and todays cases = 36818 The difference between 3/27 hospitalized projections and todays hospitalizations = 12977 The difference between 3/27 deaths projections and todays deaths = 2978 . I generally prefer to look at the hospitalization data because it&#39;s the least-wrong measurement we have. The number of confirmed cases via COVID-19 tests is undercounting how many people have the virus. Probabaly badly undercounting it. On the other hand, most of the people who have a severe case of the disease will end up in hospital. So I&#39;m going to use it as my main way to measure the course of the outbreak. . Take a look at the orange dots and lines on the graph below. See where the COVID hospitalization curve below starts to bend over to a shallower slope? A bit more than 2 wks after the stay at home order! The case curve doesn&#39;t bend before that, but this is likely becuase we still had a huge backlog of tested people waiting for their results back then. . You can also see this bending of the curve in the doubling time graphs in the last section of this notebook: We see the first increases in hospitalization doubling time are on April 4th or so. In the last weeks of March the estimated doubling time was around 4 to 5 days for all metrics: cases, hospitalizations, ICU patients, and deaths. Today the doubling time for these is around 15 - 25 days. . Let&#39;s talk about deaths. If you want to see how many people would be dead today if we’d kept doubling every 4 days throughout April do this: find today’s date on the x-axis, go straight up until you find the faded red dash-dot line, then go straight left and read the number off the y-axis. . If you do this, you’ll get the same answer I did: . #collapse print(&#39;{} people would be dead today of COVID-19 without social distancing.&#39;.format(orig_projected.loc[today,&#39;Deaths projected on 3/27&#39;]) ) print(&#39;Instead we have just {} dead, because our doubling time is now about {} days, nwhich is shown by the much shallower-sloped dashed lines projecting forward from today.&#39;.format( actual.loc[today,&#39;Deaths history&#39;], int(doubling_time_hosp) ) ) . . 3089 people would be dead today of COVID-19 without social distancing. Instead we have just 111 dead, because our doubling time is now about 19 days, which is shown by the much shallower-sloped dashed lines projecting forward from today. . The curve has flattened dramatically. Stay-at-home and social distancing are working!! . Clearly we can’t assign credit to any particular action, whether by individuals or the government. We only know that the complete package has worked. Please keep in mind that in the days ahead we will be running an experiment on ourselves, loosening one restriction or another, waiting for a few weeks to see if we get a return to bad doubling times. If so, restrictions may return. . The main point of this letter is that all the pain we’ve experienced saved many lives. Here in San Diego, it’s clearly true. I hope it’s true across our state and nation as well. COVID-19 is not the flu, and without some combination of social distancing, stay-at-home, and various closures we would be in deep trouble. . Just how much trouble? Without those interventions, today the number of people projected to be hospitalized (orange faded dash-dot line) exceeds the number of hospital beds we have available. You can see more of this analysis in the section below. . If that&#39;s correct, San Diego today could look like northern Italy or New York City’s worst hit hospitals, with patients lying in hallways largely unattended by medical staff. That’s a recipe sure to increase the death toll. . TL;DR SOCIAL DISTANCING WORKS!!@!#! . I will note one caveat about the death projections above. Back in mid March we didn&#39;t have enough death data to get a really good estimate of the growth rate by 3/26. In fact, the death data projection includes data through all of March, not stopping at the 26th like other estimates. As you can see from the rest of the history deaths seem to be reported (but not necessarily happen) in batches. Apparently doctors have 8 days after a patient dies to file paperwork, and I&#39;m guessing some of them like to wait and send all those papers in at once. Because of this trouble, you could also consider an alternate method for projecting deaths: Just take. the current death rate of people in the hospital for COVID-19 (about 14%) and multiply it by the projected hospitalizations from March 26th. This yields a different projection on COVID-19 deaths today: . #collapse print(&#39;Based on March 26 hospitalization predictions for today, and today &#39;s fraction of hospitlaized who died nan alternate prediction of dead COVID-19 patients today without social distancing:&#39;) print(&#39;{:d}&#39;.format(int( orig_projhosp.loc[today] * frac[&#39;Deaths&#39;].iloc[-1]))) . . Based on March 26 hospitalization predictions for today, and today&#39;s fraction of hospitlaized who died an alternate prediction of dead COVID-19 patients today without social distancing: 2180 . Another alternate prediction of the death toll would be to go back to the March death data, and instead of taking the most likely doubling time, use the upper bound of the 95% confidence interval. . If you&#39;re unfamiliar with a confidence interval, this estimate says &quot;Look, we know there&#39;s noise in these death measurements, and that means the most likely fit to the data might still be wrong. But assuming all this data is really coming from an unchanging process (the doubling time isn&#39;t changing), then 95% of the time the true value for the situation will be within this interval around the most likley answer&quot; . So we will try to be as optimistic as we can, assuming that the shallowest slope/largest doubling time inside the confidence interval was the correct one to describe the epidemic in March. That prediction is: . #collapse doubling_time_death_orig, ci95, orig_death_p_est, orig_death_p_cov = estimate_doubling_time(deaths[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) upperbound_exp_param = np.log(2)/(ci95+ doubling_time_death_orig) best_projdeath = pd.DataFrame( exfunc(range(0,daysfwd),orig_death_p_est[0],upperbound_exp_param), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) print(&#39;Based on the upper 95% confidence interval of doubling time predicted on data from March 19-26 nan alternate prediction of dead COVID-19 patients today without social distancing:&#39;) print(&#39;{:d}&#39;.format(int( best_projdeath.loc[today] ))) . . Based on the upper 95% confidence interval of doubling time predicted on data from March 19-26 an alternate prediction of dead COVID-19 patients today without social distancing: 2250 . But that number is still bad, and most importantly, as soon as we have more patients in hospital with COVID-19 then we can reasonably treat then the percentage of people who will die from this disease is going to go up quickly. Which is what I&#39;ll address in teh next section... . The number of available hospital beds . A pessimistic view of the number of hospital beds that are available to COVID-19 patients is provided by KPBS reporting on March 20th: &quot;As of 2018, San Diego had 6,180 total hospital beds. Of those, about 68% were occupied at any given time. That leaves 1,950 beds, including 800 intensive care unit beds, available for new patients.&quot; . A more optimistic number of beds would include the federal hospitals helping out (such as the Navy facility in Balboa park) helping out, and a lower occupancy rate of beds because hospitals are cancelling many surgeries that can be postponed. My estimate of the number of federal beds is the number in this article - the 6,180 number above =&gt; 2,434. Based on statements from Dr. Wilma Wooten in the San Diego County COVID-19 briefing on April 22, I think that the reduction in hospital use is giving about 63% of the beds occupied with non-COVID patients, instead of 68%. Those statements also said there are currently 6509 non-federal beds. So my optimistic number of beds is going to be to use all beds in San Diego, and assume an even better 60% non-COVID occupancy rate for both county and federal hospitals: . $(6509 + 2434) * (1 - 0.6) sim 3600$ beds left over for COVID-19 patients . #collapse # Pessimistic number of county only (ignores federal) beds/icu beds available on March 20th from kpbs article above availbeds = 1950 availicu = 800 # optimistic bed capacity based on # 1) all beds including federal beds in navy hospitals (https://www.sandiegouniontribune.com/news/health/story/2020-03-16/regions-hospitals-could-face-thousands-of) # 2) reduce hospital utilization from 50% to 30% by cancelling procedures # 3) increase of 250 beds from palomar hospital expansion # 3) add 250 beds from UCSD alternate care site (WAG from me. There is no statement on how many beds this would be) availbedsO = 3600 print(&#39;pessimistic estimate of capacity: 1850 noptimistic estimate of capacity: {}&#39;.format(availbedsO)) . . pessimistic estimate of capacity: 1850 optimistic estimate of capacity: 3600 . #collapse # this projection is based on doubling time calculated only over the last week of data daysfwd = 120 fromdate = (pd.Timestamp(today)-pd.Timedelta(&#39;6 days&#39;)).isoformat().split(&#39;T&#39;)[0] startproj = (pd.Timestamp(today)+pd.Timedelta(&#39;1 day&#39;)).isoformat().split(&#39;T&#39;)[0] daystoproj = (pd.Timestamp(&#39;2020-03-19&#39;) + pd.Timedelta(&#39;{} days&#39;.format(daysfwd)) - pd.Timestamp(fromdate)).days print(&#39;Calculate exponential parameters on data from {} to {}, then project forward using those parameters for {} days&#39;.format(fromdate,today,daystoproj)) doubling_time, ci95, case_p_est, case_p_cov = estimate_doubling_time(cases[fromdate:]) doubling_time, ci95, orig_case_p_est, orig_case_p_cov = estimate_doubling_time(cases[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time, ci95, hosp_p_est, hosp_p_cov = estimate_doubling_time(hospitalizations[fromdate:]) doubling_time, ci95, orig_hosp_p_est, orig_hosp_p_cov = estimate_doubling_time(hospitalizations[&#39;2020-03-19&#39;:&#39;2020-03-26&#39;]) doubling_time, ci95, icu_p_est, icu_p_cov = estimate_doubling_time(icu[fromdate:]) hospital_capacity = days_to_value(hosp_p_est,availbeds) hospital_capacityO = days_to_value(hosp_p_est,availbedsO) orig_hospital_capacity = days_to_value(orig_hosp_p_est,availbeds) orig_hospital_capacityO = days_to_value(orig_hosp_p_est,availbedsO) print(&#39;Before social distancing had enough time to work, hospitals were projected to reach capacity on {}&#39;.format(pd.Timestamp(&#39;2020-03-20&#39;) + pd.Timedelta(&#39;1 day&#39;)*orig_hospital_capacity)) print(&#39;Now I think hospitals reach capacity on {}&#39;.format(pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacity)) print(&#39;Or optimistically they reach capacity on {}&#39;.format(pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacityO)) icu_capacity = days_to_value(icu_p_est, availicu) print(&#39;ICUs reach capacity on {}&#39;.format(pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*icu_capacity)) projhosp = pd.DataFrame(exfunc(range(0,daystoproj),hosp_p_est[0],hosp_p_est[1]), index=pd.date_range(start=fromdate,periods=daystoproj) ).astype(int) projhosp.columns = [&#39;Hospitalized projected from today&#39;] projected = projhosp[startproj:] orig_projhosp = pd.DataFrame(exfunc(range(0,daysfwd),orig_hosp_p_est[0],orig_hosp_p_est[1]), index=pd.date_range(start=&#39;2020-03-19&#39;,periods=daysfwd) ).astype(int) orig_projhosp.columns = [&#39;Hospitalized projected on 2020-03-27&#39;] orig_projected = orig_projhosp[&#39;2020-03-27&#39;:] actual = hospitalizations.rename(&#39;History of hospitalizations&#39;) actual.index = pd.DatetimeIndex(actual.index) fig, ax = plt.subplots(figsize=(10, 5)) projected.plot( # title=(&#39;Social distancing has shifted the date we could run out of hospital beds backwards by at least a month n&#39;+ # &#39;Projections from March 27 hadn &#39;t yet shown the effects of social distancing and have steeper slopes than today &#39;s projections. n&#39; + # &#39;Each projection is based on 1 week of data preceeding the date the projection is made&#39; # ) ax=ax,logy=True,linestyle=&#39;--&#39;,color=colors[1]); orig_projected.plot(ax=ax,logy=True,color=colors[1],linestyle=&#39;-.&#39;,alpha=0.3) actual.plot(ax=ax,logy=True,color=colors[1],marker=&#39;.&#39;,linestyle=&#39;&#39;); ts1 = pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacity ts2 = pd.Timestamp(fromdate) + pd.Timedelta(&#39;1 day&#39;)*hospital_capacityO plt.ylabel(&#39;Number of people&#39;); shiftvline = (pd.Timestamp(fromdate) - pd.Timestamp(&#39;2020-03-19&#39;)).days xmn = ax.get_xbound()[0] xmx = (hospital_capacity + xmn + shiftvline) ymn = 0 ymx = availbeds ax.plot([xmn, xmx], [ymx,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.annotate(&#39;Pessimistic hospital capacity&#39;, xy=(xmn+1, 2300), xycoords=&#39;data&#39;, fontsize=12,fontweight=&#39;heavy&#39;); # original capacity date xmx = orig_hospital_capacity + xmn ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.annotate(&#39;Prediction made before nsocial distancing took effect nApril 15 - April 18&#39;, xy=(xmx+4, 10), xycoords=&#39;data&#39;, fontsize=12,fontweight=&#39;heavy&#39;); xmn = ax.get_xbound()[0] xmx = (hospital_capacityO + xmn + shiftvline) ymn = 0 ymx = availbedsO ax.plot([xmn, xmx], [ymx,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); ax.annotate(&#39;Optimistic hospital capacity&#39;, xy=(xmn+1, 4200), xycoords=&#39;data&#39;, fontsize=12,fontweight=&#39;heavy&#39;); ax.annotate(&#39;Current prediction n{} - {}&#39;.format(ts1.strftime(&#39;%B %e&#39;), ts2.strftime(&#39;%B %e&#39;)), xy=(xmx-18, 10), xycoords=&#39;data&#39;, fontsize=12,fontweight=&#39;heavy&#39;); # original capacity date xmx = orig_hospital_capacityO + xmn ax.plot([xmx, xmx], [ymn,ymx], linestyle=&#39;:&#39;, color=&#39;k&#39;, alpha=0.5); lims = list(plt.ylim()) lims[1] = 2.0*10**5 plt.ylim(lims) plt.ylabel(&#39;Number of people&#39;, fontsize=14,fontweight=&#39;heavy&#39;); plt.yticks([1,10,100,1000,10000,100000],[1,10,100,&#39;1,000&#39;,&#39;10,000&#39;,&#39;100,000&#39;],fontsize=14,fontweight=&#39;heavy&#39;) for tick in ax.xaxis.get_majorticklabels(): tick.set_size(14) tick.set_weight(&#39;heavy&#39;) for tick in ax.xaxis.get_minorticklabels(): tick.set_size(14) tick.set_weight(&#39;heavy&#39;) plt.legend(loc=&#39;best&#39;,prop={&#39;weight&#39;:&#39;heavy&#39;, &#39;size&#39;:12.25},labelspacing=0.3, facecolor=&#39;w&#39;, framealpha=0.66)#, bbox_to_anchor=(0., .5, 1., 0.5)) plt.savefig(&#39;Plots/hospitalcapacity-compare.png&#39;,dpi=300); . . Calculate exponential parameters on data from 2020-04-20 to 2020-04-26, then project forward using those parameters for 88 days Before social distancing had enough time to work, hospitals were projected to reach capacity on 2020-04-15 14:19:38.844414724 Now I think hospitals reach capacity on 2020-05-24 19:11:18.293683712 Or optimistically they reach capacity on 2020-06-11 02:33:53.106941776 ICUs reach capacity on 2020-06-06 15:37:44.721523915 . When there are more patients than beds, people die more often than if there was good care available, this is what&#39;s happened in hard hit parts of NYC and northern Italy. In Italy the death rate is about 30%, here its closer to 3%. And it&#39;s a good bet that most of that is due to the way their hospitals were completely overwhelmed. . And assume social distancing didn&#39;t happen. Then today we be out of beds and San Diego hospitals would be in crisis... take a look at the graph above... that 1850 number is the pessimistic hospital capacity line, 3600 is the optimisitc one. You can see that if we followed the same path we were on in early March, today we would have many more people in hospital than there were available beds, even under the optimistic estimate. The current situation might be like in Italy: patients lying in the hallways, left largely unattended because there aren&#39;t enough people to care for them. . Now what about the path we are now? The doubling time of hospitlaization is much slower right now, but it&#39;s still not flat. But we have flattened the curve, and bought ourselves enough time to add hospital capacity so we can avoid a NYC situation. When the projections are flat, horizontal lines then we will be able to breathe a sigh of relief. . That&#39;s the point at which its safe to relax the stay at home orders and social distancing in small steps. . And yet even then we must be careful. It is clear that the disease could come roaring back into our region again. We must be vigilant, and ready to put into place some control measures once again if necessary. . N.B. My hospital capacity estimates might be optimistic. . I ran my estimates by two people who actually know stuff. . My summary: Hospitals could be stressed and not dealing well with the infected as soon as they have a couple of patients. They will then adjust and just make-do as best they can, shutting down services, moving resources from here to there as much as they can. These decisions will be made differently by each hospital so there&#39;s really no way to predict hospital capacity. . Perhaps you can see my capacity limit estimate as an upper limit... that is the hospital system in San Diego will likely be in crisis well before my estimate of late May (NB: originally this was mid April, but doubling time is slowing dramatically now!) . To read the actual exchange, which took place on March 26th, see below . --Jason . Gerald Pao, an MD/PhD who&#39;s worked in virology... . Although this is a reasonable estimate at face value it does not take into account how a hospital works You cannot have the highly contagious population in the same space as the rest of the patients who are there for other reasons. So you need to ask how many barrier nursing negative pressure beds does the hospital have? The answer is for any normal hospital the number of beds is between 5-10 I have asked my friends in CA from the Bay area, LA and SD and no one has given me a number higher than 10 This means people will have to designate hospital wings to this task and somehow separate things. In places in Spain and Italy they sometimes designated particular hospitals for this task or had the ER designated for it and rearranged it. In other ones they had particular floors closed off etc. so the real capacity will depend on what the hospital administrators decide to do and will be made in uneven and unpredictable ways as there is no single set of rules on how to manage this. Therefore the pressure on hospitals and when you will be out of capacity will be difficult to estimate. We do not have a national healthcare system as you know so it’s gonna be a a free for all. . Gerald Pao . The Salk Institute for Biological Studies . Joel Wertheim, a bioinformatician who specializes in the evolution and epidemiology of HIV... . Hi Jason, . Interesting stuff. . Another point to consider in the general design is the time-lag between all of these states. Most people are not admitted to the hospital for quite some time after infection. If they proceed to the ICU and/or death, that also can take weeks after initial infection/diagnosis. I’m not sure of the proportion of cases in San Diego diagnosed after admittance to the hospital, as opposed to be people who aren&#39;t (yet) sick enough to be hospitalized. . Also, at UC San Diego, negative pressure rooms were originally used for COVID patients (back when we were treating the first patients brought back from China). Now, they are just being used for procedures that would result in high likelihood of spread (i.e., intubation). So the number of beds can change with the severity of the epidemic (as Gerald also mentioned). . Cheers, . Joel . University of California San Diego .",
            "url": "https://jasongfleischer.github.io/argo-navis/covid19/jupyter/epidemiology/2020/04/20/SanDiego-COVID-19.html",
            "relUrl": "/covid19/jupyter/epidemiology/2020/04/20/SanDiego-COVID-19.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Buzz Aldrin's lifetime mean altitude",
            "content": "At some point online I saw someone calculate Neil Armstrong&#39;s mean lifetime altitude. This seems like a complete Randall Monroe of xkcd.com kind of thing. But I cannot for the life of me find the link now to put it here. . Anyway I needed a silly toy problem to test the Jupyter notebook to blog post capability of fastpages now that I&#39;m using it. So here I am... extending the Armstrong problem to the case of a still-alive astronaut who&#39;s lifetime mean altitude is slowly decreasing. . # per wiki he graduated USMA in 1951 and by 1972 at retirement. I made up the graduation date # he already had 3500 hours flying time per https://www.nationalaviation.org/our-enshrinees/aldrin-buzz/ # assume 9km average flight altitude (~30kft) AFstart = pd.Timestamp(&#39;May 1, 1951 12:00:00 UTC&#39;) AFend = pd.Timestamp(&#39;July 1, 1971 12:00:00 UTC&#39;) AFdur = AFend - AFstart AFeffectiveAlt = 9 * pd.Timedelta(&#39;3500 hours&#39;) / AFdur # spread flight hours over the whole AF career evenly # Gemini 12 https://en.wikipedia.org/wiki/Gemini_12 # assume mean orbital distance of apogee and perigee G12start = pd.Timestamp(&#39;November 11, 1966, 20:46:33 UTC&#39;) G12end = pd.Timestamp(&#39;November 15, 1966, 19:21:04 UTC&#39;) G12meanAlt = np.mean([160,270]) # already in km from wiki # Apollo 11 https://airandspace.si.edu/sites/default/files/images/5317h.jpg # https://history.nasa.gov/SP-4029.pdf # Table 7.11 of https://www.hq.nasa.gov/alsj/a11/a11MIssionReport_1971015566.pdf MoonMeanDistKm = 384400 A11liftoff = pd.Timestamp(&#39;July 16, 1969, 13:32:00 UTC&#39;) A11EarthOrbitAlt = 100 * NMi2Km # this is rough A11TLI = A11liftoff + pd.Timedelta(&#39;2 hours 50 minutes&#39;) A11MidcourseOut = A11liftoff + pd.Timedelta(&#39;26 hours 45 minutes&#39;) A11MidcourseAlt = 109475 * NMi2Km A11LunarInsert = A11liftoff + pd.Timedelta(&#39;75 hours 50 minutes&#39;) A11TEI = A11liftoff + pd.Timedelta(&#39;135 hours 24 minutes&#39;) A11splashdown = A11liftoff + pd.Timedelta(&#39;195 hours 18 minutes&#39;) # lets check to see if we need to do any detailed interpolation, or if mean is enough... # print(109475 * NMi2Km / MoonMeanDistKm, (A11MidcourseOut - A11TLI) / (A11LunarInsert - A11MidcourseOut)) # &gt; 0.5274393860561915 0.4872665534804754 # this is close enough to linear interpolation that I&#39;m not going to bother with anything else # so for the duration of the flight from earth to moon, just fill in 1/2 the lunar distance baidx = pd.date_range(start=&#39;January 20, 1930&#39;,end=&#39;January 20, 2020&#39;,freq=&#39;H&#39;) BuzzAlt = pd.Series(0,index=baidx) BuzzAlt[AFstart:AFend] = AFeffectiveAlt BuzzAlt[G12start:G12end] = G12meanAlt BuzzAlt[A11liftoff:A11TLI] = A11EarthOrbitAlt BuzzAlt[A11TLI:A11LunarInsert] = MoonMeanDistKm / 2 BuzzAlt[A11LunarInsert:A11TEI] = MoonMeanDistKm BuzzAlt[A11TEI:A11splashdown] = MoonMeanDistKm / 2 . BuzzAlt.plot(logy=True, ylim=(1,1000000)); plt.ylabel(&#39;kilometers&#39;); plt.xlabel(&#39;on this date&#39;); plt.title(&#39;Aldrin &#39;s altitude history&#39;); plt.annotate(&#39;Gemini 12&#39;, xy=(G12start,2*10**2), xycoords=&#39;data&#39;, xytext=(-55,0), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Apollo 11&#39;, xy=(A11liftoff,4*10**5), xycoords=&#39;data&#39;, xytext=(5,0), textcoords=&#39;offset points&#39;); . BuzzAlt.expanding().mean().plot(logy=True,ylim=(0.0001,1000)); plt.ylabel(&#39;kilometers&#39;); plt.xlabel(&#39;as of this date&#39;); plt.title(&#39;Aldrin &#39;s mean altitude over his lifetime at a given time&#39;); plt.annotate(&#39;Gemini 12&#39;, xy=(G12start,10**-1), xycoords=&#39;data&#39;, xytext=(-55,0), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Apollo 11&#39;, xy=(A11liftoff,10**2), xycoords=&#39;data&#39;, xytext=(-50,0), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Air Force nflight hours&#39;, xy=(AFstart,10**-2), xycoords=&#39;data&#39;, xytext=(-60,-10), textcoords=&#39;offset points&#39;); . # same thing, but linear y-axis instead of log BuzzAlt.expanding().mean().plot(); plt.ylabel(&#39;kilometers&#39;); plt.xlabel(&#39;as of this date&#39;); plt.title(&#39;Aldrin &#39;s mean altitude over his lifetime at a given time&#39;); plt.annotate(&#39;Gemini 12&#39;, xy=(G12start,10**-1), xycoords=&#39;data&#39;, xytext=(-55,10), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Apollo 11&#39;, xy=(A11liftoff,10**2), xycoords=&#39;data&#39;, xytext=(-50,10), textcoords=&#39;offset points&#39;); plt.annotate(&#39;Air Force nflight hours&#39;, xy=(AFstart,10**-2), xycoords=&#39;data&#39;, xytext=(-60,10), textcoords=&#39;offset points&#39;); .",
            "url": "https://jasongfleischer.github.io/argo-navis/funny/jupyter/xkcd/2020/02/29/Buzz-Aldrin's-average-altitude.html",
            "relUrl": "/funny/jupyter/xkcd/2020/02/29/Buzz-Aldrin's-average-altitude.html",
            "date": " • Feb 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Did the 80s or the 90s have better movies?",
            "content": "80s v 90s . This is a stupid project I did in 2015. After I got in an argument with friends about movies, I used MATLAB to webscrape data off of IMDB to prove that 80s movies rule. Then I copy/pasted the .m file into a document, and put the rendered plots into the document to make a psuedo-notebook, like the cool kids who used Mathematica got to play with. . Here’s what I learned: . I was wrong, so wrong about the 80s being best. | A friend told me that I was an idiot to use MATLAB for this, and I should really look into this python &amp; Jupyter notebooks stuff. | Manually configuring a webscraper is damned hard. Dedicated tools like Beautifulsoup are soooo nice! | If you want, the .m/.mat files are in this Github repo. . If for some reason the embedded pdf viewer above doesn’t work for you, just go ahead and click through to be awfully amused and saddened that someone wasted their time with this .",
            "url": "https://jasongfleischer.github.io/argo-navis/movies/matlab/2020/02/01/Were-the-80s-or-90s-better-at-making-classic-movies.html",
            "relUrl": "/movies/matlab/2020/02/01/Were-the-80s-or-90s-better-at-making-classic-movies.html",
            "date": " • Feb 1, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Publications",
          "content": "L Chow, ENC Manoogian, A Alvear, JG Fleischer, H Thor, K Dietsche, Q Wang, JS Hodges, KS Nair, S Panda, DG Mashek. (2020) Effects of time restricted eating on body composition and metabolic measures in overweight humans: a randomized trial. Obesity, 28: 860--869. https://doi.org/10.1002/oby.22756 | MJ Wilkinson, ENC Manoogian, A Zadourian, H Loa, S Fakourib, A Shoghib, JG Fleischer, S Navlakha, S Panda, PR Taub. (2019) Ten-hour time-restricted eating reduces weight, blood pressure, and atherogenic lipids in patients with metabolic syndrome. Cell Metabolism,31(1):92-104, https://doi.org/10.1016/j.cmet.2019.11.004 | AT Hutchison, P Regmi, ENC Manoogian, JG Fleischer, GA Wittert, S Panda, LK Heilbronn. (2019) Time-restricted feeding improves glucose tolerance in men at risk of type 2 diabetes: a randomized crossover trial. Obesity https://doi.org/10.1002/oby.22449 | JG Fleischer, R Schulte, HH Tsai, S Tyagi, A Ibarra, MN Shokhirev, L Huang, MW Hetzer, S Navlakha (2018). Predicting age from the transcriptome of human dermal fibroblasts. Genome Biology, 19:221. https://doi.org/10.1186/s13059-018-1599-6 | GP Dunster, L de la Iglesia, M Ben-Hamo, C Nave, JG Fleischer, S Panda, HO de la Iglesia (2018). Sleepmore in Seattle: Later school start times are associated with more sleep and better performance in high school students. Science Advances, 4 (12), eaau6200. https://dx.doi.org/10.1126/sciadv.aau6200 | JL McKinstry, JG Fleischer, Y Chen, WE Gall, GM Edelman (2016). Imagery may arise from associations formed through sensory experience: a network of spiking neurons controlling a robot learns visual sequences in order to perform a mental rotation task. PLOS One, https://dx.doi.org/10.1371/journal.pone.0162155 | JG Fleischer (2014). Persistent activity through multiple mechanisms in a spiking network that solves DMS tasks. Computational and Systems Neuroscience Meeting (COSYNE). | JG Fleischer and JA Gally (2013). A spiking neural network model of working memory can solve delayed match-to-sample tasks and displays a serial position effect. Poster at The Society for Neuroscience Annual Meeting. | JG Fleischer and AE Kozarev (2012). Perceptual grouping and figure-ground segre- gation arising from short-term plasticity in a spiking network. Computational and Systems Neuroscience Meeting (COSYNE) . | JG Fleischer, JL McKinstry, DE Edelman, and GM Edelman (2011). The Case For Using Brain-Based Devices To Study Consciousness. In, JL Krichmar and H Wagatsuma (Eds.), Neuromorphic and Brain-Based Robots: Trends and Perspectives, Cambridge University Press, pp. 303–320. | JG Fleischer and GM Edelman (2009). Brain-based devices: An embodied approach to linking nervous system structure and function to behavior. IEEE Robotics &amp; Automation Magazine, 16(3):33–41. https://dx.doi.org/10.1109/MRA.2009.933621 | JG Fleischer and JL Krichmar (2007). Sensory integration and remapping in a medial temporal lobe model during maze navigation by a brain-based device. Journal of Integrative Neuroscience, 6(3):403–431. http://dx.doi.org/10.1142/S0219635207001568 | JG Fleischer, JA Gally, GM Edelman, and JL Krichmar (2007). Retrospective and prospective responses arising in a modeled hippocampus during maze navigation by a brain-based device. Proceedings of the National Academy of Sciences USA, 104(9):3556–3561. https://doi.org/10.1073/pnas.0611571104 | DA Nitz, WJ Kargo, and JG Fleischer (2007). Dopamine signaling and the distal reward problem. Neuroreport, 18(17):1833–1836. https://doi.org/10.1097/WNR.0b013e3282f16d86 | JG Fleischer (2007). Neural correlates of anticipation in cerebellum, basal ganglia, and hippocampus. In, MV Butz, O Siguard, G Baldassarre, G Pezzulo (Eds.),, Anticipatory Behavior in Adaptive Learning Systems: From Brains to Individual and Social Behavior, Lecture Notes in Artificial Intelligence. vol 4520, pp.19–34. | JG Fleischer, JA Gally, GM Edelman, and JL Krichmar (2007). Different neural pathways lead to journey-dependent and journey-independent place cell activity in an embodied model of hippocampus. Poster at The Society for Neuroscience Annual Meeting. | JG Fleischer, B Szatmary, DB Hutson, DA Moore, JA Snook, GM Edelman, and JL Krichmar (2006). A neurally controlled robot competes and cooperates with humans in Segway Soccer. In Proceedings of the IEEE International Conference on Robotics and Automation, pp.3673–3678 | B Szatmary, JG Fleischer, DB Hutson, DA Moore, JA Snook, GM Edelman, and JL Krichmar (2006). A Segway-based human-robot soccer team. In Proceedings of the IEEE International Conference on Robotics and Automation, pp.4436–4438. | JL Krichmar, AK Seth, DA Nitz, JG Fleischer, and GM Edelman (2005). Spatial navigation and causal analysis in a brain-based device modeling cortical-hippocampal interactions. Neuroinformatics, 3(3):197–222. https://doi.org/10.1385/NI:3:3:197 | JG Fleischer (2004). Imitation is not enough for lexicon learning In Proceedings of the Eighth International Conference on Simulation of Adaptive Behavior, pp.477– 486. | JG Fleischer, SR Marsland, and JL Shapiro (2003). Sensory Anticipation for Autonomous Selection of Robot Landmarks. In, MV Butz, O Siguard, P Gerard (Eds.), Anticipatory Behavior in Adaptive Learning Systems: Foundations, Theories, and Systems, Lecture Notes in Artificial Intelligence. vol 2684, pp.201–221. | JG Fleischer and SR Marsland (2002). Learning to autonomously select landmarks for navigation and communication. In Proceedings of the Seventh International Conference on Simulation of Adaptive Behavior, pp. 151–160. | JG Fleischer and UDF Nehmzow (2001). Towards robots that give each other navigational directions: Learning symbols for perceptual categories. In Proceedings of the 3rd British Conference on Autonomous Mobile Robotics and Autonomous Systems. Dept. of Computer Science, University of Manchester Technical Report UMCS-01-4-1. | JG Fleischer and WO Troxell (1999). Biomimicry as a tool in the design of robotic systems. In Proceedings of the 3rd International Conference on Engineering Design and Automation. Integrated Technology Systems, Prospect, KY. | .",
          "url": "https://jasongfleischer.github.io/argo-navis/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jasongfleischer.github.io/argo-navis/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}